{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data (MNIST)\n",
    "\n",
    "Here, we use a suitable object that will hold the data and that will be easier to be referenced from within the MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset(object):\n",
    "    \n",
    "    def __init__(self,x,y,test_size):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=test_size, random_state=0)\n",
    "        self.xtrain = xtrain.T\n",
    "        self.xtest = xtest.T\n",
    "        self.ytrain = ytrain.T\n",
    "        self.ytest = ytest.T\n",
    "                \n",
    "        self.nx = self.xtrain.shape[0]\n",
    "        self.mtrain = self.xtrain.shape[1]\n",
    "        self.mtest = self.xtest.shape[1]\n",
    "\n",
    "        self.indices = np.arange(self.mtrain)\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.counter = 0\n",
    "        print(\"Training Data: x=%s | y=%s, Test Data: x=%s | y=%s\"%(str(self.xtrain.shape), str(self.ytrain.shape), \n",
    "                                                                    str(self.xtest.shape), str(self.ytest.shape)))\n",
    "\n",
    "                    \n",
    "    def next_batch(self, batchsize):\n",
    "        if (self.counter+1)*batchsize >= self.mtrain:\n",
    "            self.counter = 0\n",
    "            np.random.shuffle(self.indices)\n",
    "        it = self.indices[self.counter*batchsize:(self.counter+1)*batchsize]\n",
    "        self.counter += 1\n",
    "        xbatch = self.xtrain[:,it].reshape(self.nx, batchsize)\n",
    "        ybatch = self.ytrain[:,it].reshape(1,batchsize)\n",
    "        return xbatch, ybatch\n",
    "\n",
    "    def training_set(self):\n",
    "        return self.xtrain, self.ytrain\n",
    "\n",
    "    def test_set(self):\n",
    "        return self.xtest, self.ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: x=(784, 60000) | y=(1, 60000), Test Data: x=(784, 10000) | y=(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "x, y = mnist['data'], np.array(mnist['target'], dtype='int')\n",
    "y = y.reshape(y.size,1)\n",
    "\n",
    "ds = Dataset(x,y,test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Components\n",
    "\n",
    "In this section, we define the different compartments of the MLP model, including \n",
    "\n",
    "* Activation function including suitable methods to compute values and derivatives: here, the simoid activation function will be used. \n",
    "* For implementing softmax, a softmax activation function is provided. It just provides the possibility to compute the softmax values, but not the derivatives. The derivatives will be needed for the backpropagation but will be integrated in the code for the softmax layer.  \n",
    "* Initializer for initializing the weights and bias parameters: here, normally distributed initial values will be provided.\n",
    "* Cost function used for training - including suitable methods for computing the values and derivatives: here, the cross-entropy cost will be used.\n",
    "* Layer: The first core component to be implemented/completed by the students.\n",
    "* MLP: The second core component to be implemented/completed by the students. Allows to configure an arbitrary number of layers into a sequential structure.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_function(object):\n",
    "    \"\"\"\n",
    "    Empty parent implementation of all activation functions. All child implementations should implement \n",
    "    the two methods defined below.\n",
    "    \"\"\"\n",
    "    def compute_value(self, z):\n",
    "        \"\"\"\n",
    "        Computes the value of the activation function element wise for input array z of arbitrary shape. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented in the child implementation.\")\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function element wise for input array z of arbitrary shape. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented in the child implementation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation_function(activation_function):\n",
    "        \n",
    "    def compute_value(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        s = self.compute_value(z)\n",
    "        return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_activation_function(activation_function):\n",
    "    \n",
    "    def compute_value(self, z):\n",
    "        expz = np.exp(z)\n",
    "        norm = np.sum(expz, axis=0)        \n",
    "        return expz / norm\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        raise NotImplementedError(\"Computation of the gradient implemented in the Softmax Layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm_Initializer(object):\n",
    "    \n",
    "    def initialize_weights(self, size, mu=0.0, sigma=1.0):\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)\n",
    "    \n",
    "    def initialize_bias(self, size, mu=0.0, sigma=1.0):\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost(object):\n",
    "    \n",
    "    def compute_value(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the value of the cost function for given labels y and predicted probs. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"\")\n",
    "        \n",
    "    def compute_derivative(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the cost function for given labels y and predicted probs. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Cost):\n",
    "\n",
    "    def compute_value(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the value of the cost function for given labels y and predicted probs.\n",
    "        \n",
    "        Arguments:\n",
    "        y -- labels, a numpy array of shape (1,m)\n",
    "        prob -- predicted probabilities for the different classes, a numpy array of shape (ny,m)\n",
    "        \n",
    "        Returns:\n",
    "        cost -- a scalar\n",
    "        \"\"\"\n",
    "        n,m = prob.shape\n",
    "        assert(np.max(y)<=n)\n",
    "        py = prob[y,np.arange(m)]\n",
    "        J = -np.sum(np.log(py))/m    \n",
    "        return J\n",
    "    \n",
    "        \n",
    "    def compute_derivative(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the cost function for given labels y and predicted probs. \n",
    "        Derivative is computed w.r.t. predicted probs.\n",
    "\n",
    "        y -- labels (shape (1,m))\n",
    "        prob -- predicted probabilities for the different classes (shape (ny,m))\n",
    "        \n",
    "        Returns:\n",
    "        Gradient of cost with respect to the predicted probabilities (shape (ny,m))\n",
    "        \"\"\"\n",
    "        n,m = prob.shape\n",
    "        result = np.zeros((n,m),dtype=float)\n",
    "        result[y[0,:],np.arange(m)] = 1.0\n",
    "        result /= prob\n",
    "        return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, layerid, nunits, nunits_prev, activ_func, initializer):\n",
    "        \"\"\"\n",
    "        Instantiates a fully connected layer for an MLP, with given number of input and output activations.\n",
    "        \n",
    "        Arguments:\n",
    "        layerid -- integer id for the layer.\n",
    "        nunits -- number of classes.\n",
    "        nunits_prev -- number of input activations.\n",
    "        activ_func -- activation function to be used (with a compute_value and compute_derivative-method).\n",
    "        initializer -- initializer for the weights and the biases.        \n",
    "        \"\"\"\n",
    "        self.layerid = layerid\n",
    "        self.nunits = nunits\n",
    "        self.nunits_prev = nunits_prev\n",
    "        self.initializer = initializer\n",
    "        self.activ_func = activ_func\n",
    "        \n",
    "        self.weights = None # shape (nunits,nunits_prev)\n",
    "        self.bias = None # shape (nunits,1)\n",
    "        self.logits = None # shape(nunits,m) - will be needed for the backprop part ...\n",
    "        self.activations = None # shape(nunits,m)\n",
    "        self.grad_logits = None # shape(nunits,m) - will be needed as basis for different gradients\n",
    "        \n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases. It uses Xavier normalisation (to be discussed in \"regularisation\").\n",
    "        \"\"\"\n",
    "        sigmaw = np.sqrt(2.0/(self.nunits+self.nunits_prev)) # suited for sigmoid activation function\n",
    "        sigmab = np.sqrt(1.0/self.nunits)\n",
    "        self.weights = self.initializer.initialize_weights(size=(self.nunits,self.nunits_prev), mu=0.0, sigma=sigmaw)\n",
    "        self.bias = self.initializer.initialize_bias(size=(self.nunits,1), mu=0.0, sigma=sigmab)\n",
    "        \n",
    "    \n",
    "    def propagate(self, activations_prev):\n",
    "        \"\"\"\n",
    "        Computes the activations of the layer given the activations of the previous layer.\n",
    "        Caches the computed logits (z-values) and activations since the values will be needed \n",
    "        when using backpropagation to compute the gradients w.r.t. weigths and biases.\n",
    "\n",
    "        Arguments:\n",
    "        activations_prev -- activations of the previous layer (or input layer). A numpy array of shape \n",
    "        (nunits_prev,m).\n",
    "        \n",
    "        Returns:\n",
    "        activations -- activations of this layer, a numpy array of shape (nunits,m)\n",
    "        \"\"\"\n",
    "        np.testing.assert_equal(activations_prev.shape[0],self.nunits_prev)\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    \n",
    "    def backpropagate(self, grad_activations):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. to the input activations (activations of the previous \n",
    "        layer a^[l-1]) of the given layer. It also computes the gradient w.r.t. the logits (z-values) of \n",
    "        the given layer. This will be needed as the basis for computing the gradient of the cost w.r.t. \n",
    "        the weights and bias of the given layer. \n",
    "        \n",
    "        The method assumes that the forward propagation (`propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent logit-values (self.logits) and activations (self.activations) are available.  \n",
    "        \n",
    "        Arguments:\n",
    "        grad_activations -- gradient of the cost w.r.t. to the output activations of the given layer (a^[l]). \n",
    "        A numpy array of shape (nunits_prev,m) \n",
    "        \n",
    "        Returns:\n",
    "        grad_activations_prev -- gradient of the cost w.r.t. to the input activations of the given layer.\n",
    "        \"\"\"\n",
    "        nsamples =  self.logits.shape[1]\n",
    "        np.testing.assert_equal(grad_activations.shape,(self.nunits,nsamples))\n",
    "        ### START YOUR CODE ###\n",
    "        # useful for testing the shapes - once the self.grad_logits is computed\n",
    "        # np.testing.assert_equal(self.grad_logits.shape,(self.nunits,nsamples))\n",
    "        \n",
    "        \n",
    "        # useful for testing the shapes - once the self.grad_activations_prev is computed\n",
    "        # np.testing.assert_equal(grad_activations_prev.shape,(self.nunits_prev,nsamples))        \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    def gradient_weights(self, activations_prev):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. the weights of the given layer and for the given mini-batch.  \n",
    "        \n",
    "        Arguments:\n",
    "        activations_prev -- activations of the previous layer (or input layer). A numpy array of shape \n",
    "        (nunits_prev,m).\n",
    "        \n",
    "        Returns:\n",
    "        grad_weights -- the gradient w.r.t. to the weights. A numpy array of shape (nunits, nunits_prev).\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "\n",
    "    def gradient_bias(self):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. the bias of the given layer and for the given mini-batch.  \n",
    "                \n",
    "        Returns:\n",
    "        grad_bias -- the gradient w.r.t. to the bias. A numpy array of shape (nunits, 1).\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def __init__(self, layerid, nunits, nunits_prev, initializer):\n",
    "        \"\"\"\n",
    "        Instantiates a Softmax layer with given number of input activations and normalised scores.\n",
    "        \n",
    "        Arguments:\n",
    "        layerid -- integer id for the layer.\n",
    "        nunits -- number of classes.\n",
    "        nunits_prev -- number of input activations.\n",
    "        initializer -- initializer for the weights and the biases.        \n",
    "        \"\"\"\n",
    "        super().__init__(layerid, nunits, nunits_prev, softmax_activation_function(), initializer)\n",
    "        \n",
    "    def backpropagate(self, grad_activations):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. to the input activations (activations of the previous \n",
    "        layer a^[L-1]) of the softmax layer. It also computes the gradient w.r.t. the logits (z-values) of \n",
    "        the softmax layer. This will be needed as the basis for computing the gradient of the cost w.r.t. \n",
    "        the weights and bias of the layer. \n",
    "        \n",
    "        The method assumes that the forward propagation (`propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent logit-values (self.logits) and activations (self.activations) are available.  \n",
    "        \n",
    "        Arguments:\n",
    "        grad_activations -- gradient of the cost w.r.t. to the output activations of the given layer (a^[l]). \n",
    "        A numpy array of shape (nunits_prev,m) \n",
    "        \n",
    "        Returns:\n",
    "        grad_activations_prev -- gradient of the cost w.r.t. to the input activations of the given layer.\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        ### END YOUR CODE ###            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, units_per_layer, activ_func, initializer, softmax_as_last_layer=True):\n",
    "        \"\"\"\n",
    "        Instantiates a (fully connected) MLP with architecture specified by the list `units_per_layer` \n",
    "        which contains the number of units for layers (including the input and the output layer).\n",
    "        Uses layerid=0 for the first hidden layer, layerid=1 for the second, etc.\n",
    "        \n",
    "        Arguments:\n",
    "        units_per_layer -- number of units for layers (including the input and the output layer)\n",
    "        activ_func -- activation function to be used in the different layers except possibly in the output \n",
    "        layer (in case softmax_as_last_layer=True)\n",
    "        initializer -- initializer for the weights and biases of all the layers.\n",
    "        softmax_as_last_layer -- flas to indicate whether the last layer should be a softmax layer.\n",
    "        \"\"\"\n",
    "        self.layers = [] # list of layers (instances of class Layer or Softmax or the like); ordered along the forward path.\n",
    "        self.number_layers = -1 # number of layers (excluding input layer, excluding softmax layer)\n",
    "        self.x = None # input data for one iteration (propagate and backpropagate) \n",
    "\n",
    "        ### START YOUR CODE ###        \n",
    "        self.number_layers = len(units_per_layer)-1 # input layer not counted as layer\n",
    "        if softmax_as_last_layer:\n",
    "            self.number_layers -= 1\n",
    "        layerid = 0\n",
    "        for i in range(self.number_layers):\n",
    "            layerid += 1\n",
    "            layer = Layer(layerid=layerid, nunits=units_per_layer[i+1], nunits_prev=units_per_layer[i], activ_func=activ_func, initializer=initializer)\n",
    "            self.layers.append(layer)\n",
    "        if softmax_as_last_layer:\n",
    "            layerid += 1\n",
    "            layer = Softmax(layerid, units_per_layer[self.number_layers+1], units_per_layer[self.number_layers], initializer)\n",
    "            self.layers.append(layer)\n",
    "        ### END YOUR CODE ###        \n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize()\n",
    "    \n",
    "    def propagate(self, x): \n",
    "        \"\"\"\n",
    "        Computes the output of the MLP for given input (by using the propagate-method). Once this method has \n",
    "        been run for a given input mini-batch, the activations and the logits of all the layers are computed \n",
    "        and cached (consistent with the mini-batch). \n",
    "        \n",
    "        Arguments:\n",
    "        x -- input of shape (n_0,m)\n",
    "        \n",
    "        Returns: \n",
    "        a -- activations of the last layer of shape (n_L,m)\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###                \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###        \n",
    "\n",
    "    def backpropagate(self, grady):\n",
    "        \"\"\"\n",
    "        Runs the backpropagation for the given MLP (after having executed the propagate-method).\n",
    "        It starts with passing in the gradient of the cost w.r.t. the activations of the last layer \n",
    "        (i.e. the input to the cost function) and ends up with the gradient of the cost w.r.t. the \n",
    "        input to the first layer (e.g. input x). Once this method has been run, the gradients w.r.t.\n",
    "        to the logits (z-values) are computed and cached. These will be used to update the weights \n",
    "        and biases in accordance with the gradient descent principle. \n",
    "        \n",
    "        Arguments:\n",
    "        grady -- gradient with respect to the output of the network the activations of the last layer\n",
    "        that is input to the cost function. A numpy array of shape (n_L,m)\n",
    "        \n",
    "        Returns:\n",
    "        gradient with respect to the input x. A numpy array of shape (n_0,m)\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###        \n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of all the layers consistent with the gradient descent principle.\n",
    "        It assumes that the propagate and backpropagate methods have been executed.\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate -- learning rate to be used in the update rule.   \n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Implementation of Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dff91b3423ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx00\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayersizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayersizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_activation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNorm_Initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x00 = np.array([0.2,0.1,-0.3, 0.2, 0.5,-1.0, 1.0,1.5,-1.0]).reshape(3,3)\n",
    "layersizes = [3,10,20,10,5]\n",
    "np.random.seed(1)\n",
    "mlp = MLP(layersizes, sigmoid_activation_function(), Norm_Initializer(), True)\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    layer = mlp.layers[layerid]\n",
    "    layer.weights = np.ones(shape=(layer.nunits,layer.nunits_prev), dtype='float')*0.1\n",
    "    layer.bias = np.zeros(shape=(layer.nunits,1), dtype='float')\n",
    "mlp.initialize()\n",
    "y00 = mlp.propagate(x00)\n",
    "np.testing.assert_equal(y00.shape, (5,3))\n",
    "y00_expected = np.array([\n",
    "    [0.18559891, 0.18553251, 0.18809197],\n",
    "    [0.28155687, 0.28171852, 0.27895721],\n",
    "    [0.05074394, 0.05071549, 0.05141478],\n",
    "    [0.13742613, 0.13757631, 0.13833757],\n",
    "    [0.34467414, 0.34445717, 0.34319847]])\n",
    "np.testing.assert_array_almost_equal(y00, y00_expected, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance of Forward Propagation\n",
    "\n",
    "Measure the runtimes for propagating all the MNIST training set (60'000 samples) with different batch sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-04dc2fa6da5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlayersizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayersizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_activation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNorm_Initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "layersizes = [ds.nx,1000,800,700,600,400,200,100,100,10]\n",
    "mlp = MLP(layersizes, sigmoid_activation_function(), Norm_Initializer(), True)\n",
    "mlp.initialize()\n",
    "batchsizes = [1,10,100,1000,10000,60000]\n",
    "runtimes = {}\n",
    "nsamples = ds.mtrain\n",
    "\n",
    "for batchsize in batchsizes:\n",
    "    start = timer()\n",
    "    nbatches = int(nsamples / batchsize)\n",
    "    for i in range(nbatches):\n",
    "        xx, yy = ds.next_batch(batchsize)\n",
    "        yypred = mlp.propagate(xx)\n",
    "    end = timer()\n",
    "    runtime = end-start\n",
    "    print(\"Exec time for batchsize %i:%s\"%(batchsize,runtime))\n",
    "    runtimes[batchsize] = runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Implementation of the Gradient\n",
    "\n",
    "For checking the implementation, compute a numeric approximation of the gradient ('numeric gradient') by using the formula as explained in the class. Compare this with the analytic formulas ('analytic gradient') obtained when implementing backprop (or the derivatives of the cross entropy cost).\n",
    "\n",
    "Actually, do this checking for the cross entropy cost as well as for the MLP model.\n",
    "\n",
    "When choosing delta0~1.0e-8, we expect a difference of the numeric and the analytic gradient of <= 3.0e-7.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1854167203395605e-08\n",
      "0.0\n",
      "1.414096573171264e-07\n"
     ]
    }
   ],
   "source": [
    "# Check Gradient of Cross Entropy Cost\n",
    "\n",
    "ce = CrossEntropy()\n",
    "\n",
    "y = np.array([0,2]).reshape(1,2)\n",
    "probs0 = np.array([[0.7,0.1,0.2]]).reshape(1,3).T\n",
    "J0 = ce.compute_value(y,probs0)\n",
    "\n",
    "delta0 = 1.0e-8\n",
    "for i in range(3):\n",
    "    delta = np.zeros((3,1),dtype='float')\n",
    "    delta[i,0] = delta0\n",
    "    probs1 = probs0 + delta\n",
    "    J1 = ce.compute_value(y,probs1)\n",
    "    numeric = (J1-J0)/delta0\n",
    "    analytic = ce.compute_derivative(y,probs0)[i,0]\n",
    "    d = np.abs(numeric-analytic)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layer with id 0\n",
      "Testing layer with id 1\n",
      "Testing layer with id 2\n",
      "Testing layer with id 3\n"
     ]
    }
   ],
   "source": [
    "# Check gradient of cost w.r.t. weights and biases of MLP\n",
    "# An output with a discrepancy is provided only if the difference between numeric and analytic gradient \n",
    "# exceeds the accuray of 3.0e-7. Definitely, the difference should not get much larger than this accuracy.\n",
    "\n",
    "accuracy = 3.0e-7\n",
    "\n",
    "layersizes = [100,200,300,100,10]\n",
    "mlp0 = MLP(layersizes, sigmoid_activation_function(), Norm_Initializer(), True)\n",
    "mlp0.initialize()\n",
    "mlp1 = MLP(layersizes, sigmoid_activation_function(), Norm_Initializer(), True)\n",
    "mlp1.initialize()\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    mlp1.layers[layerid].weights = mlp0.layers[layerid].weights.copy()\n",
    "    mlp1.layers[layerid].bias = mlp0.layers[layerid].bias.copy()\n",
    "\n",
    "m = 2\n",
    "x = np.random.uniform(-0.5,0.5,size=(layersizes[0],m))\n",
    "y = np.random.randint(0,3,size=(1,m))\n",
    "probs0 = mlp0.propagate(x)\n",
    "gradJ = ce.compute_derivative(y,probs0)\n",
    "mlp0.backpropagate(gradJ)\n",
    "J0 = ce.compute_value(y,probs0)\n",
    "\n",
    "delta0 = 1.0e-8\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    print(\"Testing layer with id %s\"%(layerid))\n",
    "    if layerid==0:\n",
    "        activations_prev = x\n",
    "    else:\n",
    "        activations_prev = mlp0.layers[layerid-1].activations\n",
    "    for i in range(layersizes[layerid+1]):\n",
    "        for j in range(layersizes[layerid]):\n",
    "            mlp1.layers[layerid].weights[i,j]+=delta0\n",
    "            probs1 = mlp1.propagate(x)\n",
    "            J1 = ce.compute_value(y,probs1)\n",
    "            numeric = (J1-J0)/delta0\n",
    "            analytic = mlp0.layers[layerid].gradient_weights(activations_prev)[i,j]\n",
    "            d = np.abs(numeric-analytic)\n",
    "            if d > accuracy:\n",
    "                print(\"Layer %i (%i,%i)\"%(layerid,i,j), d, numeric, analytic)\n",
    "            mlp1.layers[layerid].weights[i,j]-=delta0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Methods\n",
    "\n",
    "May be helpful for implementing the training including the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(y, prob):\n",
    "    \"\"\"\n",
    "    Computes the error rate for given labels and per class scores.\n",
    "    \n",
    "    Arguments:\n",
    "    y -- labels, a numpy array of shape (1,m) with values between 0 (incl.) and n (excl.).\n",
    "    prob -- predicted probabilities, a numpy array of shape (n,m)\n",
    "    \n",
    "    Returns:\n",
    "    Rate of incorrectly classified samples (out of m) - i.e. with the highest probability predicted \n",
    "    not for the correct class.   \n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "    ypred = np.argmax(prob, axis=0).reshape(1,m)\n",
    "    rate = np.sum(y != ypred) / m\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_and_error(x,y, model, cost):\n",
    "    prob = model.propagate(x)\n",
    "    J = cost.compute_value(y, prob)\n",
    "    e = error_rate(y,prob)\n",
    "    return J,e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost, error:  2.794667871317479 2.7975184982345964 0.9141666666666667 0.9106\n",
      "0 cost, error:  0.5780180364019608 0.5782685922685432 0.1358 0.133\n",
      "1 cost, error:  0.4285854524380106 0.43428401488898655 0.10851666666666666 0.113\n",
      "2 cost, error:  0.36244622792047604 0.3712763148184501 0.09396666666666667 0.0983\n",
      "3 cost, error:  0.3203929315968549 0.33181418005624913 0.08465 0.0904\n",
      "4 cost, error:  0.2987202077278259 0.31350859523327146 0.07918333333333333 0.0843\n",
      "5 cost, error:  0.27773067887775044 0.2909990864800288 0.07535 0.0814\n",
      "6 cost, error:  0.264561115991118 0.2800010269699008 0.07126666666666667 0.0772\n",
      "7 cost, error:  0.25422235465454707 0.271392790539135 0.06878333333333334 0.0765\n",
      "8 cost, error:  0.24151413638111008 0.26135273430864714 0.06711666666666667 0.0741\n",
      "9 cost, error:  0.231311507019221 0.249531224391825 0.06246666666666666 0.0704\n",
      "10 cost, error:  0.22017163631505723 0.24264154044900324 0.061233333333333334 0.067\n",
      "11 cost, error:  0.20993870160562872 0.23307043474541306 0.05801666666666667 0.0647\n",
      "12 cost, error:  0.20832563523651437 0.22964284536652912 0.05673333333333333 0.0638\n",
      "13 cost, error:  0.19772844795612685 0.22128005289543462 0.05505 0.064\n",
      "14 cost, error:  0.19184159185171412 0.2198809229368615 0.05235 0.0642\n",
      "15 cost, error:  0.1905961548000388 0.2161435532486456 0.05295 0.0611\n",
      "16 cost, error:  0.18787645068699488 0.21343074103559798 0.0518 0.0619\n",
      "17 cost, error:  0.18223029547565392 0.21143739571365933 0.05103333333333333 0.0602\n",
      "18 cost, error:  0.17769882569951756 0.20468452959912534 0.04986666666666666 0.0584\n",
      "19 cost, error:  0.17178213872903808 0.20006503919208157 0.04808333333333333 0.0569\n",
      "20 cost, error:  0.1729706763258614 0.2020306696365276 0.04883333333333333 0.0563\n",
      "21 cost, error:  0.16351541708764006 0.19409561060778197 0.0455 0.0543\n",
      "22 cost, error:  0.1644918658833928 0.1941192429392621 0.04535 0.0553\n",
      "23 cost, error:  0.16410428201829225 0.19153511471323253 0.045566666666666665 0.0546\n",
      "24 cost, error:  0.15953602289789556 0.1898992937559707 0.043916666666666666 0.0561\n",
      "25 cost, error:  0.15307365215310303 0.1838191297398822 0.042366666666666664 0.0534\n",
      "26 cost, error:  0.14810402967492575 0.1833320719242726 0.04065 0.0533\n",
      "27 cost, error:  0.14595050721707584 0.18129056203828467 0.04135 0.0541\n",
      "28 cost, error:  0.14763376114291105 0.18201192090588303 0.04061666666666667 0.0522\n",
      "29 cost, error:  0.1454681985285412 0.1846724936464299 0.040016666666666666 0.0532\n",
      "30 cost, error:  0.14092012187161188 0.17590747277114113 0.03961666666666667 0.0516\n",
      "31 cost, error:  0.13539168134776566 0.17409832940783362 0.037233333333333334 0.05\n",
      "32 cost, error:  0.13458512166248018 0.17552406225650152 0.036816666666666664 0.0536\n",
      "33 cost, error:  0.13508485463552625 0.17169191692590446 0.03695 0.0501\n",
      "34 cost, error:  0.13049682699913123 0.16741996097303707 0.035916666666666666 0.0481\n",
      "35 cost, error:  0.13020170000325848 0.17045607880097974 0.03548333333333333 0.049\n",
      "36 cost, error:  0.12722131520549676 0.1661707989799651 0.03456666666666667 0.0484\n",
      "37 cost, error:  0.12642360663246469 0.16879640189078518 0.03473333333333333 0.049\n",
      "38 cost, error:  0.11986220750749514 0.16197703407070665 0.03265 0.0491\n",
      "39 cost, error:  0.12325640783227507 0.16071158001116956 0.033883333333333335 0.0482\n",
      "40 cost, error:  0.1209028397857924 0.16116857282528418 0.0334 0.0467\n",
      "41 cost, error:  0.11653681898556478 0.1604007591828175 0.03188333333333333 0.0456\n",
      "42 cost, error:  0.11911061844054396 0.16188415098691442 0.0324 0.0476\n",
      "43 cost, error:  0.11421273208789434 0.15848874250695485 0.030883333333333332 0.0466\n",
      "44 cost, error:  0.11094788227592267 0.159862153440445 0.03015 0.0467\n",
      "45 cost, error:  0.11589218279098315 0.1611077542887405 0.03175 0.0487\n",
      "46 cost, error:  0.11429860573850062 0.15778707207302 0.03191666666666667 0.0462\n",
      "47 cost, error:  0.11137393948879236 0.1554285970181465 0.030966666666666667 0.0467\n",
      "48 cost, error:  0.10792731455017347 0.15117343583461934 0.029433333333333332 0.0448\n",
      "49 cost, error:  0.10649366252615917 0.15321444495979553 0.02915 0.0459\n",
      "50 cost, error:  0.10539190482190061 0.150882160135977 0.0282 0.0451\n",
      "51 cost, error:  0.10597388301875335 0.1499926638759395 0.02885 0.0463\n",
      "52 cost, error:  0.10422144359989635 0.15060894744669812 0.028583333333333332 0.0457\n",
      "53 cost, error:  0.1024982443099379 0.14927006987899039 0.0278 0.0439\n",
      "54 cost, error:  0.10385453425835767 0.14845963399602743 0.0286 0.0437\n",
      "55 cost, error:  0.10317696420348431 0.14930349720744335 0.028533333333333334 0.0438\n",
      "56 cost, error:  0.10097037738620392 0.14738473003197472 0.028366666666666665 0.0443\n",
      "57 cost, error:  0.0997831539259831 0.15470929341912862 0.027216666666666667 0.0466\n",
      "58 cost, error:  0.10074639868115981 0.14997084199025626 0.027766666666666665 0.045\n",
      "59 cost, error:  0.09838049789081114 0.14605909790757546 0.026466666666666666 0.0432\n",
      "60 cost, error:  0.09781566296005313 0.15054221058836043 0.02675 0.0442\n",
      "61 cost, error:  0.09396023162596472 0.1435179870239872 0.025683333333333332 0.0445\n",
      "62 cost, error:  0.09313064015586968 0.14304897905611266 0.024866666666666665 0.0414\n",
      "63 cost, error:  0.09303779391274257 0.14667588229388667 0.025416666666666667 0.0454\n",
      "64 cost, error:  0.09309280856429245 0.14559204128029124 0.024816666666666667 0.0425\n",
      "65 cost, error:  0.09083249517130884 0.14117170106188026 0.02415 0.0417\n",
      "66 cost, error:  0.09126335061896113 0.14320704812559956 0.024516666666666666 0.0436\n",
      "67 cost, error:  0.09145968039672128 0.1469076352598633 0.023966666666666667 0.0425\n",
      "68 cost, error:  0.08929755022298369 0.14782642073945718 0.024166666666666666 0.0434\n",
      "69 cost, error:  0.08879121211441099 0.14548524991694006 0.023966666666666667 0.0435\n",
      "70 cost, error:  0.08634391208616603 0.13916742130487333 0.023 0.0413\n",
      "71 cost, error:  0.08716307607871088 0.1407318662973031 0.023383333333333332 0.0425\n",
      "72 cost, error:  0.08672591050698988 0.14099130625310155 0.02395 0.0425\n",
      "73 cost, error:  0.08536885717479693 0.1401429656216343 0.022616666666666667 0.0407\n",
      "74 cost, error:  0.08479113570878269 0.1414624679392961 0.022766666666666668 0.0419\n",
      "75 cost, error:  0.08083813836143793 0.13761213101385975 0.021116666666666666 0.0403\n",
      "76 cost, error:  0.08148294505563393 0.13989639431524187 0.021333333333333333 0.04\n",
      "77 cost, error:  0.07754985505905791 0.13349555034366825 0.019866666666666668 0.0398\n",
      "78 cost, error:  0.07831576089525177 0.13408713870044234 0.021416666666666667 0.0401\n",
      "79 cost, error:  0.07456687613997993 0.1343578287474089 0.019033333333333333 0.0414\n",
      "80 cost, error:  0.07802668792739754 0.13614499471852157 0.0211 0.0408\n",
      "81 cost, error:  0.07510431275520697 0.13721189984029852 0.019283333333333333 0.0412\n",
      "82 cost, error:  0.07777391113820152 0.13849741049392644 0.021016666666666666 0.0421\n",
      "83 cost, error:  0.07546036843051677 0.13572717541025514 0.019683333333333334 0.0406\n",
      "84 cost, error:  0.0748600706191619 0.13406178660362766 0.019533333333333333 0.0418\n",
      "85 cost, error:  0.06983602043936606 0.1323467663285299 0.01825 0.0402\n",
      "86 cost, error:  0.06935867118794549 0.13344067880719038 0.018033333333333332 0.0408\n",
      "87 cost, error:  0.07310381295539574 0.13678399518976328 0.01895 0.0407\n",
      "88 cost, error:  0.07207627525979912 0.13319852178911348 0.019216666666666667 0.0398\n",
      "89 cost, error:  0.07131440015246943 0.13480176170254993 0.01875 0.0403\n",
      "90 cost, error:  0.06938442166632447 0.1317210689573712 0.018316666666666665 0.0397\n",
      "91 cost, error:  0.06858606491856518 0.13275724951128803 0.017616666666666666 0.0387\n",
      "92 cost, error:  0.06837672217611755 0.13598540478008853 0.018216666666666666 0.0393\n",
      "93 cost, error:  0.06474316381061253 0.13372728904851805 0.016583333333333332 0.0398\n",
      "94 cost, error:  0.06510018267806089 0.1327659007295895 0.016783333333333334 0.039\n",
      "95 cost, error:  0.0632139190603677 0.1308463179733981 0.015733333333333332 0.0379\n",
      "96 cost, error:  0.06462289494198013 0.13321276815994026 0.016366666666666668 0.0386\n",
      "97 cost, error:  0.06350681481450568 0.1336608119239457 0.01595 0.0397\n",
      "98 cost, error:  0.06357997693353953 0.1315647628491152 0.0161 0.0389\n",
      "99 cost, error:  0.06622467710646421 0.1319101134474761 0.017533333333333335 0.0397\n",
      "189.77956882701255\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "learning_rate = 0.01\n",
    "batchsize = 100\n",
    "nbatches = int(ds.mtrain / batchsize)\n",
    "nepochs = 100\n",
    "\n",
    "layersizes = [ds.nx,100,10]\n",
    "mlp = MLP(layersizes, sigmoid_activation_function(), Norm_Initializer(), True)\n",
    "ce = CrossEntropy()\n",
    "\n",
    "# for computing the cost and the error rates on the training and test set once per epoch \n",
    "# we need access to the whole training and test set  \n",
    "xtrain, ytrain = ds.training_set()\n",
    "xtest, ytest = ds.test_set()\n",
    "\n",
    "# initial cost and error rate (training and test set)\n",
    "Jtrain, etrain = cost_and_error(xtrain, ytrain, mlp, ce)    \n",
    "Jtest, etest = cost_and_error(xtest, ytest, mlp, ce)    \n",
    "print(\"cost, error: \", Jtrain, Jtest, etrain, etest)\n",
    "\n",
    "# prepare learning curves (train and test, cost and error rate)\n",
    "epoch_cost_train = [Jtrain]\n",
    "epoch_error_train = [etrain]\n",
    "epoch_cost_test = [Jtest]\n",
    "epoch_error_test = [etest]\n",
    "\n",
    "# loop over the epochs and the mini-batches, update the parameters and fill the values for the learning curves\n",
    "start = timer()\n",
    "mlp.initialize()\n",
    "for iepoch in range(nepochs):\n",
    "    for ibatch in range(nbatches):\n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        \n",
    "        ### END YOUR CODE ###        \n",
    "    \n",
    "    xtrain, ytrain = ds.training_set()\n",
    "    Jtrain, etrain = cost_and_error(xtrain, ytrain, mlp, ce)    \n",
    "    epoch_cost_train.append(Jtrain)\n",
    "    epoch_error_train.append(etrain)\n",
    "    xtest, ytest = ds.test_set()\n",
    "    Jtest, etest = cost_and_error(xtest, ytest, mlp, ce)    \n",
    "    epoch_cost_test.append(Jtest)\n",
    "    epoch_error_test.append(etest)\n",
    "\n",
    "    print(iepoch, \"cost, error: \", Jtrain, Jtest, etrain, etest)\n",
    "\n",
    "end = timer()\n",
    "lapsetime = end-start\n",
    "print(lapsetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(nepochs+1), epoch_cost_train, label=\"train\")\n",
    "plt.plot(range(nepochs+1), epoch_cost_test, label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = nepochs\n",
    "plt.axis([0,xmax,0.0,0.4])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEKCAYAAADTgGjXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VFX6wPHvSe8JqYQEJBBaQu8gAkoHRVBAQMCODXVdG7ruurq6q/uzrxXBXlEsKKCIgKBI7z0hlIQkpEBCQnpyfn+cwRQSypDJJJn38zw8ZO49994z8wx5Oe09SmuNEEIIYQ0ne1dACCFEwyVBRAghhNUkiAghhLCaBBEhhBBWkyAihBDCahJEhBBCWM2mQUQpNVIptU8pFa+Uml3N+YFKqc1KqRKl1IQq525QSsVZ/txgy3oKIYSwjrLVOhGllDOwHxgGJAEbgCla690VyrQE/IAHgYVa668sxwOBjUBPQAObgB5a6xM2qawQQgir2LIl0huI11onaK2LgM+BqysW0Fof0lpvB8qqXDsC+FlrfdwSOH4GRtqwrkIIIazgYsN7RwCJFV4nAX0u4tqIqoWUUjOBmQDe3t492rdvX+MNE9JzaVaShIerMwS3Oc9qCCFE47Zp06YMrXWItdfbMoioao6db9/ZeV2rtZ4DzAHo2bOn3rhxY403nPXpZq458Heu8E+Ge2suJ4QQjkQpdfhirrdld1YS0LzC60gguQ6urVZTPw8OF/ujT6aA5AsTQohaYcsgsgFoo5SKUkq5AZOBhed57U/AcKVUE6VUE2C45ZjVmvp7cLQ0AFWSDwVZF3MrIYQQFjYLIlrrEmAW5pf/HmC+1nqXUuoppdRYAKVUL6VUEjAReFsptcty7XHgX5hAtAF4ynLMamF+HhzRoeZFRvzF3EoIIYSFLcdE0FovBhZXOfaPCj9vwHRVVXftu8C7tVWXpv4e7CyLMi9StkLzXrV1ayFEA1ZcXExSUhIFBQX2ropNeXh4EBkZiaura63e16ZBpD5p6udBMkEUugbgnrLN3tURQtQTSUlJ+Pr60rJlS5Sqbk5Pw6e1JjMzk6SkJKKiomr13g6T9iTUzx1QHPNuBxJEhBAWBQUFBAUFNdoAAqCUIigoyCatLYcJIu4uzgR6u5HgGg1pe6Ck0N5VEkLUE405gJxmq/foMEEETJfWHqKgrNgEEiGEEBfFsYKIvwebCi3LT1K327cyQggBZGVl8cYbb1zwdaNHjyYry/7LFRwqiIT5ebA1twm4+cq4iBCiXqgpiJSWlp71usWLFxMQEGCrap03h5mdBaY7KyOvhLK2nXCSICKEqAdmz57NgQMH6Nq1K66urvj4+BAeHs7WrVvZvXs348aNIzExkYKCAu677z5mzpwJQMuWLdm4cSO5ubmMGjWKAQMGsGbNGiIiIvjuu+/w9PSsk/o7VBAJ9/cAIDcwFr+dH0NpCTg71EcghDiLJ7/fxe7kk7V6z5hmfjxxVWyN55999ll27tzJ1q1bWblyJWPGjGHnzp1/TsV99913CQwMJD8/n169enHttdcSFBRU6R5xcXF89tlnvPPOO0yaNIkFCxYwbdq0Wn0fNXGo7qzWod4AJLq3gZJ8yIyzc42EEKKy3r17V1rL8eqrr9KlSxf69u1LYmIicXFn/t6Kioqia9euAPTo0YNDhw7VVXUdqyUSHeoLwE4dRSyYcZHQDnatkxCi/jhbi6GueHt7//nzypUrWbZsGX/88QdeXl4MHjy42rUe7u7uf/7s7OxMfn5+ndQVHKwl4u/pSpifOxtyAsHFUwbXhRB25+vrS05OTrXnsrOzadKkCV5eXuzdu5e1a9fWce3OzaFaIgBtQn3Zn14ATTtKEBFC2F1QUBCXXnopHTt2xNPTk7CwsD/PjRw5krfeeovOnTvTrl07+vbta8eaVs/xgkiYD19sSET37oLaMR/KysDJoRpkQoh65tNPP632uLu7O0uWLKn23Olxj+DgYHbu3Pnn8QcffLDW63c2Dvfbs02oL3lFpZzw7wCFJ+HEQXtXSQghGizHCyJhPgAccGltDkiXlhBCWM3xgkioCSLbCsPBxQOSNti5RkII0XA5XBAJ8HIjxNedfemFENkLDq22d5WEEKLBcrggAqY1sj8tF6IGQupOyLuonXeFEMJhOWwQiT+Wg245ANBweI29qySEEA2SYwaRMF9OFZWS4h1jFh1Kl5YQwk6sTQUP8PLLL5OXl1fLNbowjhlELIPrcceLoUUfOChBRAhhHw09iDjcYkMwLRGAuGM5DGo5AJY/DacywTvoHFcKIUTtqpgKftiwYYSGhjJ//nwKCwsZP348Tz75JKdOnWLSpEkkJSVRWlrK3//+d44dO0ZycjKXX345wcHBrFixwi71d8ggEujtRrCPG3HHcqH3QHPw8G8Qc7V9KyaEsK8lsyF1R+3es2knGPVsjacrpoJfunQpX331FevXr0drzdixY1m1ahXp6ek0a9aMRYsWASanlr+/Py+++CIrVqwgODi4dut8ARyyOwsgOtSHuLQciOgOrl5w6Dd7V0kI4eCWLl3K0qVL6datG927d2fv3r3ExcXRqVMnli1bxiOPPMLq1avx9/e3d1X/5JAtETDpT77dchTt5IJq0VfGRYQQZ20x1AWtNY8++ii33377Gec2bdrE4sWLefTRRxk+fDj/+Mc/7FDDMzlsS6RtmA85hSUcO1kILS+D9D2Qm27vagkhHEzFVPAjRozg3XffJTc3F4CjR4+SlpZGcnIyXl5eTJs2jQcffJDNmzefca29OGxL5PQGVXFpOTRteZk5ePg3iB1vx1oJIRxNxVTwo0aNYurUqfTr1w8AHx8fPv74Y+Lj43nooYdwcnLC1dWVN998E4CZM2cyatQowsPD7TawrrTWdnlwbevZs6feuHHjeZfPyC2k59PL+NvoDtx2aXN4riV0vg6ufNF2lRRC1Dt79uyhQwfH2OG0uveqlNqkte5p7T0dtjsr2MedVsHerDmQAc6ucMmlsG8xFJ+59aQQQojqOWwQARjYNoQ/EjIpKC6F/rMgJwU2zLV3tYQQosFw6CAyqF0IBcVlrD943CRjbH0FrH4BCk7au2pCiDrUWLr1z8ZW79Ghg0jfqCDcXJz4db9lVtaQf0D+cfjjNftWTAhRZzw8PMjMzGzUgURrTWZmJh4eHrV+b4ednQXg6eZMn6hAft2fzt8BmnUzs7PWvAa9bgOfEHtXUQhhY5GRkSQlJZGe3rin+Ht4eBAZGVnr93XoIAIwqG0ITy/aQ9KJPCKbeMHlj8PuhbD6eRj1nL2rJ4SwMVdXV6KiouxdjQbLobuzAAa3M62NVfszzIHgaOg+HTbMg5PJdqyZEELUfzYNIkqpkUqpfUqpeKXU7GrOuyulvrCcX6eUamk57qqU+kAptUMptUcp9ait6tg6xIeIAE9+3Z9WfvDS+6CsGLZ/YavHCiFEo2CzIKKUcgZeB0YBMcAUpVRMlWK3ACe01tHAS8Dp/qOJgLvWuhPQA7j9dICxQT0Z1C6E3+MzKSopMwcDW0HzvrDtc2jEg21CCHGxbNkS6Q3Ea60TtNZFwOdA1VzrVwMfWH7+ChiilFKABryVUi6AJ1AE2Gze7aC2IeQWlrD5yInyg10mQ/peSNlqq8cKIUSDZ8sgEgEkVnidZDlWbRmtdQmQDQRhAsopIAU4AjyvtT5e9QFKqZlKqY1KqY0XM7Oif+sgXJxU+VRfgNhx4OwO26RLSwghamLLIKKqOVa1b6imMr2BUqAZEAU8oJRqdUZBredorXtqrXuGhFg/HdfXw5UelzTh593HyueKezaBdqNgx5dQWmz1vYUQojGzZRBJAppXeB0JVJ3u9GcZS9eVP3AcmAr8qLUu1lqnAb8DVicIOx/jukUQn5ZbpUtrCuRlQPwvtny0EEI0WLYMIhuANkqpKKWUGzAZWFilzELgBsvPE4Dl2jQFjgBXKMMb6AvstWFduapLM7zdnPlk3ZHyg9FDwCsYtn1my0cLIUSDZbMgYhnjmAX8BOwB5mutdymlnlJKjbUUmwcEKaXigb8Cp6cBvw74ADsxweg9rfV2W9UVwMfdhau7RbBoewrZeZbuK2dX6DQB9i2B/BNnv4EQQjggm64T0Vov1lq31Vq31lo/Yzn2D631QsvPBVrriVrraK11b611guV4ruV4rNY6Rmv9f7as52lTe7egsKSMr7cklR/sMhlKC2HJbMhKrPliIYRwQA6/Yr2ijhH+dIn059N1R8oH2MO7mjxaO76EV7rAgtsgzaY9a0II0WBIEKliap8WxKXlsvGwpftKKRjzPNy3DfreaTauenc45KTat6JCCFEPSBCp4qouzfB1d+HTigPsAAHNYcQzMHOl2f1wycP2qJ4QQtQrEkSq8HJzYVy3CBbtSCErr+jMAsFtYNDDsPs72PND3VdQCCHqEQki1ZjcuzlFJWUs3FZDFt9L74OwjrD4QSjIrtvKCSFEPSJBpBqxzfyJCffjy41J1RdwdoWxr0LuMfj5ibqtnBBC1CMSRGowoUckO45msze1hryPET2g712w6T1IsekSFiGEqLckiNRgXLcIXJ0VX9XUGgEY+BC4esH6OXVXMSGEqEckiNQg0NuNK9qH8u3WoxSXllVfyDMAOk2EHV/JinYhhEOSIHIWE3s0JyO3iJX7zpJmvtetUJIPWz+tu4oJIUQ9IUHkLAa1CyHYx42vNp0l3Ul4Z2jeBzbMhbIaWixCCNFISRA5C1dnJ8Z3i+CXPWlk5hbWXLDXbXA8ARJW1F3lhBCiHpAgcg4TejSnpEzzxcaztEZixoJ3iGmNCCGEA5Egcg7tmvoyuF0Iry+PJyU7v/pCLu7QfQbs/xGyjlRfRgghGiEJIufhqbEdKSnTPPX97poL9bjJ/L1hXt1USggh6gEJIuehRZAX9w5pw5KdqSzfe6z6QgHNocNVZvFhYU7dVlAIIexEgsh5uu2yVrQJ9eHv3+4ir6ik+kL97zW5tLZ8XLeVE0IIO5Egcp7cXJx4Znwnjmbl8/KyuOoLRfaEFv3gjzegtIZAI4QQjYgEkQvQOyqQKb1bMGdVAvNrmq3V/x7IPgJ7vqvbygkhhB1IELlAT46N5bI2wcxesJ2fdlWzu2HbURDYGn5/FU5vsSuEEI2UBJEL5ObixFvTetCleQD3fLaFNQcyKhdwcoJ+d0PKVjj8u30qKYQQdUSCiBW83V1478ZetAzy4rYPNpKcVWX9SJcp4BUEv79inwoKIUQdkSBipQAvN96e3pNTRaV8X3UHRDcv6DcL4pbCnu/tU0EhhKgDEkQuQlSwN50j/Vm8I+XMk/3vgaad4Yf74VRm3VdOCCHqgASRizS6UzjbkrJJOpFX+YSzK4x7E/KzYMlD9qmcEELYmASRizS6YzgAS3ZUM1OraUcY9AjsXAC7F9ZxzYQQwvYkiFykFkFedIzwY1F1XVoAA/4C4V1Mt1bqjrqtnBBC2JgEkVowulM4WxOzOFp1lhaYbq3xbwMa3h4IP/wV8o6Xny/Oh/R9sP8nWDcHVj5rUqcIIUQD4GLvCjQGYzqF898f97FkRwq3XtbqzAKhHWDWRhMgNsw13Vsh7eDEIcitJqFjSQEM/aeNay2EEBdPWiK14JIgb2Kb+VU/S+s0r0AY/V+44ze45FJwdoM2w+Dyx+Gad+CWn+GB/RB7Dax/R2Z0CSEaBGmJ1JLRncL5v5/2kZyVT7MAz5oLhsXAlE9rPj/oEdj1DfzxGgx9ovYrKoQQtUhaIrVkdCczS+uDNYfQF5MzK7Q9xI6D9XMqj50IIUQ9JEGklkQFe3N112a8vSqBB77cRkFxqfU3G/gwFJ2CP14vP5bwq9k1UZI6CiHqEenOqkUvTepKq2AfXlq2n7hjubw9vcfZu7ZqEhYDMVfDurehRV+Tg+vQanPu5FEY8o/arbgQQljJpi0RpdRIpdQ+pVS8Ump2NefdlVJfWM6vU0q1rHCus1LqD6XULqXUDqWUhy3rWhucnBT3DW3D3Bk9OZRxiklv/0FRSZl1Nxv0MBTlwCcTIH0vjHwWut8Aq1+AtW/VbsWFEMJKNmuJKKWcgdeBYUASsEEptVBrvbtCsVuAE1rraKXUZOA54DqllAvwMTBda71NKRUEFNuqrrVtaEwY/5vajRvf28A3W5K4rleLC79JWCwMfwbKSqDXreDuA2WlkJcJP84GnxDoeG3tV14IIS6ALVsivYF4rXWC1roI+By4ukqZq4EPLD9/BQxRSilgOLBda70NQGudqbW+iEGGujeobQidIvx5c+UBSkqtbI30n2VWvLv7mNdOznDtXLMF79e3m3ESIYSwI1sGkQig4h6ySZZj1ZbRWpcA2UAQ0BbQSqmflFKblVIPV/cApdRMpdRGpdTG9PT0Wn8DF0Mpxd2Xt+ZQZh6Ld1aTV8tarp4w5TMIag3zp0P6/tq7txBCXCBbBhFVzbGqU4tqKuMCDACut/w9Xik15IyCWs/RWvfUWvcMCQm52PrWuuExTYkO9eGNFfGUldXirCrPAJg63yxY/GQCnMo49zVCCGEDtgwiSUDzCq8jgeSayljGQfyB45bjv2qtM7TWecBioLsN62oTTk6Kuwa3Zm9qDsv3ptXuzZtcAlM+N2lTPp8KxQW1e38hhDgPtgwiG4A2SqkopZQbMBmomg99IXCD5ecJwHJtVur9BHRWSnlZgssgYDcN0NguzYhs4slrK+IvbhFidSJ7muSOietgwS1Q2mDmHgghGgmbBRHLGMcsTEDYA8zXWu9SSj2llBprKTYPCFJKxQN/BWZbrj0BvIgJRFuBzVrrRbaqqy25ODtxx6DWbE3M4qVlcbUfSGLHwcjnYO8PMH8GlBTW7v2FEOIsVK3/UrOTnj176o0bN9q7GtUqKS3j4QXb+XrzUcZ0DueFiV3wcHWu3YesfwcWPwjRw+C6j8wAvBBCnINSapPWuqe110vakzrg4uzECxO7MHtUexbvSGHS239w7GQtj2H0vg2uehXil8FH15jpv2VWTi0WQojzJEGkjiiluGNQa+ZM70l8Wi5T3lnL8VNFtfuQHjeYMZJju+DDsfC/brDqeZOHSwghbECCSB0bFhPG+zf15uiJfG56bz2nCktq9wFdroMH95k9Svybw/J/wTd3nH/ixuICWHArfDerduslhGiUJIjYQe+oQF6f2p2dySe54+NN1ufXqomrJ3SeBDf+AMP+BXsWwto3KpcpK4XcKgs0i/Lg8ymw40vY8pHpGhNCiLOQIGInQ2PCePaaTqyOy+Chr7bZ7kH974H2V8LP/4Aja82x1J0wdwg8Hw0fXAVxP0NhLnw6CQ6sgDEvQmAr+PExKK3llpIQolGRIGJHE3s25/6hbfluazIr9tXyYsTTlIJxb0BAC/jyRlj2T5gzCLISod8syIg3q95faAeHfzdjKr1ugeFPQ8Y+2PSebeolhGgUzhlElFLOSqn/q4vKOKI7B7cmKtibp3/YTbG1iRrPxcMfJn0I+Sfgt5dM9t+718OIZ+C+bTDuLWjWDSa8Z8ZUANqNhqiBsOIZc11VpSWw5RPY/qVt6iyEaBDOGUQs2XN7WLLrilrm5uLE30Z34ED6KT5Ze9h2D2raCaZ/CzMWwjVzwDvIHHdxg65TzPhJ7Ljy8krBiP9AQTasfLZ8unBZmQkcr/eC7+6Cr2+FPT9YX689P0DmAeuvF0LY1fnuJ7IF+E4p9SXw53xRrfXXNqmVgxnSIZTL2gTz0rI4xnWLIMDLDYCyMo2TUy3G7kv6XVj5ph2h+wxY95bZ893dD5xcIC8DQmNh0kfw+8vwze0QtAxCO5jr8rPM1r5thkHz3jXf/8Qhk4m45QC44Xur35YQwn7Oa8W6Uqq6jnGttb659qtknfq8Yv187EvNYdQrq5jRryXjukXwxYYjLNyaTHSYL/+b3I0WQV72qVjRKdj6qUn0WJANhTkQPRRirwEnJziZDHMGg6sX3LbcDN4v+ivkpICrN8z4tuZAsvRxWPM/8/Mdv5nWkhCiTl3sinVJe1KPPP7tDj5eewQAT1dnhsaE8eu+NLSG5yZ0ZnSncDvXsAaJ6+H9MeAZCLmpppUy9AmzA+OpTLhhITTrWvmaolPwYgeI6GECT+w1MO51+9RfCAdWJ2lPlFKRSqlvlFJpSqljSqkFSqlIax8qqvfAsHZc0z2Cf4/vxPq/DeF/U7qx6N7LaB3qw12fbObRr3eQnlMPEyw27w1XvgxFuXD54zBzJbQdYcZfPPzho/FwrEoS5u3zTctm4MPQdSrsmA+5NpqhJoSwmfPtzvoZ+BT4yHJoGnC91nqYDet2QRpDS6QmRSVl/N9Pe5n320HcXJyY1ucSZg5qRaivh72rVllZmeniquh4Arw32ixuvGkxBLcxq+ff6AfOLnD7asiMh9d6wqDZcPmj9qm7EA6qrhIwhmit39Nal1j+vA/Uv60EGyk3Fyf+NiaGZX8dxOhO4bz7+0EG/ncFH6w5VPup5S9G1QACZtHijIWAhg/GmsH0Q6shfQ/0vt3MAgtuA21GwMZ5srmWEA3M+QaRDKXUNMuaEWel1DQg05YVE2dqFeLDi5O68ssDg+nXKognFu5i1mdbyCmo55tRhbQ104tL8s0K+V//a8ZPOk0oL9P3TjiVDju/sl89hRAX7HyDyM3AJCAVSMHsQlhvZmY5mqhgb+bd0ItHRrbnx52pjH3td/amnrR3tc6uaUeY9rWZ/ntotck4XHHPk1aDITQGfn9VWiNCNCDntWIduFZrPVZrHaK1DtVaj9Na23BlnDgXJyfFnYNb8+mtfThVWML176wj8Xievat1dhHd4fqvoO0o6HNH5XNKwZAnTKqVH2fbp35CiAt2vgPrK7XWg21fHes15oH1czmQnss1b6wh2MeNr++8FH8v1z/P5RQU4+vhepar65mfnzALGMe/DV0mm2OlJbD7W3DzMWtUnCuskc2IN1mKc9PMbK+CbLP3fN+7wLWeTTwQoh6qk3UiSqlnAH/gCyqvWN9s7YNrmyMHEYC1CZlMn7eOHpc04cOb+xCXlsNry+P5cVcqz13bmUk9m9u7iuentAQ+GgdJG+G2X0zeriWPwLGd5rxPU+h2PTSJgm2fmaSRAG6+4Blgusgy9kPAJTDi39B+jGnl1Ia4ZbDhHRPgPANq555C2FldBZEV1RzWWusrrH1wbXP0IALwzZYk7v9iGy2DvDiUmYevuws+Hi64uzjxywODca7NFCq2lHMM3h4IxflQmG021xr2FDi7mX1O4paCLjMzv7pNN+tMfJuWX5+wEpbMNjPA2gyHa+eBh9/F1el4Arw92NSn120w5vmLu58Q9YTNg4hSygmYoLWeb+1D6oIEEeONlfG89/shpve9hBv6t2RNfAZ3frKZN67vXn9XvFfn8BqTk6vr9dD/XnCrkPblZLJJq9Kse82tjNIS02pY+jiEd4VpC2puPeSmwcFVEDOuclfZacX5MHcYZCdC68th17emlRTR4+LfpxB2VlctkVVa64HWPqQuSBCpXmmZZsgLK/H3dOXbuy/F4ZIx710E82+AsFiY/g14BVY+X1II746A5C0Q3gWufv3MHF7f3Q1bPoap86FFX3itN/iEwm0rqg86QjQgdbXY8Gel1INKqeZKqcDTf6x9qKg7zk6K2wa2YltSNmsTjtu7OnWv/RiY/Cmk7TGLHU+mVD7/46MmgPS/tzyZ5LInYecC2DAPFj9sAsjAh0wqFw9/GPkfSN0OG+aae+SfgG1fQMKvdf72KtEa0veZv4WoI+fbEjlYzWGttW5V+1WyjrREalZQXMqA55bTMcKf9286S2r2xiz+F/hiGrh4mNZG+9Emf9fXt5kAMvxfkHccfnrMDNhX1G4MXPcRODmb11rDx9eaxJPNe8PBX6GsxIzZ3LgYmvc6vzql74Ntn0OrQdByYPUr/i/Ehrmw6AHoPRNG/bf2JhSIRk2y+FpIEDm711fE838/7WPJfZfRIfzMQWatdePv6krfDwtuMa2IztfBnu/Njo4zFlaZNhxngoKHP3hYZnxV/WyOJ8BbA83mXh3GmqnH399rFkrOXAl+lvGnrESzJXGzrtDr1vIFlru+ge9mmaSVYGaTdZsOvW+zbubX8YPw5qVm7OhUugmMw56SQCLOyaZBRCn1sNb6v5afJ2qtv6xw7t9a68esfXBtkyBydtl5xfR79hd83F1oHeJDqJ87Xm7OHDmex6GMPNJzC7nn8mjuGdLG3lW1rZJCWP4vs4+JdyjcsbryzK4LUVwALu7lv6iP7Ya5QyG0vWmR7FsE399v0r2UFoFvOAx80PzC/+M1iOwN17wNSZtgy4dmcD+ip9llsuJq/nMpKzPpZFK3w51rzDqbDXMvPqFlcT788pRZcxPQQKaIiwtm6yCyWWvdverP1b22Nwki5/bz7mN8syWJtJOFpOUUkldUQmQTL1oGeZGVX8zKfen848oYbh4QZe+q2t7RTaaVEdS6du+753vTbRbYGo4fsASKOWa8Zfm/4Mgfplzv22H402Z74krXToeYsTDh/fPv3lo3B5Y8BGNfg+7TTVBZeA9s/dg8o/891r2XrZ/Ct3eaWWuTPrDuHqLeu9ggcq6pJaqGn6t7Leq5YTFhDIsJq/ZcSWkZsz7dwlM/7MbP05UJPRr5djG2mp7b4SoY/Bj8+iwMesTsl+LsAoFRcNMSSFgBpcVmkL66a4f/y0xLXv4UDP3nuZ+XtgeWPQHRw6DbNHPMyQnGvmq6ypb+3XSVxYytfF1+FqRsM39Sd5gxotjxlcts+dj8vftbOLrZpK0RoopzBRFdw8/VvRYNmIuzE69M6cot72/kkQXb8XF3YWRHK7t5HN3gR6DfXeDuW/m4UtD6HOtz+80y+6v89pKZBBA1CJpcYlbql+SbtC55mXBghUn3krQB3P3hqlcqj384OcP4t+DkUfh6JvhHmMBZXACr/gu/v2LGfcA8J36ZCUTuPuZY5gGTDWDAX2HzB6Zba8a3tfcZiUbjXN1ZpZg0JwrwBE5n+FOAh9a63iRlku6s2nGqsIRp89ax82g2b03rwZAO1bdchA2VFsNnUyD+5woHFWf8vy28ixnU7zTRBJrq5KbD3CFmfGP0f2H5M5DxuRgiAAAe3ElEQVQZB12mQueJ0LQLnDhoygz9Jwy431y37J8m0Ny/20wC+OlRmPGdybZcVd5xk5om5mrocGXlc4U5cGC5KVOQBSVFpsvNr5kVH4ywBZmdZSFBpPZk5xczY9469qTk8Nb07lzRXgJJnSsrMzPAsg6ZjbxyjoGbt5kx5hlgZpU1aXl+90rfZ1bcF2aDfwu46mWIHlK5zMfXmvUy9203LZOXYs2MsqlfmNbL/3qAbxjc+kvlFk9pCXwywXTTAfS40eQsc/GE7V+YrrbcY5WfFdEDbv4JnOvN/0Edmq3HRIQD8vd05cNb+jB93jru+Ggzb0/vweXtQ+1dLcfi5ATB0ebPxQppZ9K+JKwwM61Od1lVNGg2zBtqdpcMbge5qeVjLK4eZpbXd3fDrq+h47Xl1y17wtx3zAtmOvPvr8Ch302gS9pgAsa1cyEo2kxkiPsJvrwRVj1feeZYYa45V2rZYM3JxeQ9u9icZ8LmpCUiapSdV8y0eevYl5rDBzf3pl/rIHtXSdjSR+MhZbvpJkvdDn/dU95aKC0xSTHTdpsgcvljJkh8c3vlhJQHV8HXt5vxlmFPQufJZ84y+/p22PEl3LLUpO1P3w/zp0P63srlapruXFZavvDTlo5uNq3Bjtc26vU20p1lIUHENrLyipj41h+kZhcw/45+1S5UFI3EkXXw7nDzc79ZMOKZyufzT5idJ9e9ZdbbODmbKcwzvq3cNVWcD6ia93MpyIY3B5hZa4Nmm1X2Lu4mk0BIW1Mm0RKgYq6GCe+ZQFRaAr88CRvfNa2bdqNq/SMATELOZU+aKdIAl/7FjBedDiR5x2HxgxDUBgbPbvABRoKIhQQR20nOyueaN9ag0Sy4sz+RTbzYdPg48347SEFxGbcMiKJ/66DGv+LdEXx4tUmlf9c6s2iyOrlpsPpFMzV40gfgHXzhzzn0O7w/BtCmxTHpQzODrKLfX4Wf/25miPW9E7662Wyt7BNmAtqUz88c2zmX4gITAI6sNVOau00zkxJKS8x057ilsPYNEwj73mEmBmx633QDjvg3pGyFL2ZA9hFzv4EPwRWPX/j7r0fqdRBRSo0EXgGcgbla62ernHcHPgR6AJnAdVrrQxXOtwB2A//UWp91AwcJIra1LzWHiW+tIdjHnQAvVzYfycLf0xV3FyfScgrp0jyAOwe1ZmiHUFycLzIHlLCf4wfNL9iuU2z/rPXvmJT+gx4xLZGqtIYf/mJ+iXs2MQHgqpfNWMkHY81U6GlfQcsB5/e83HT4fCokrYfIXmbjMzATCDITzMQDMPcf8R8zHqW1SdK57k2T2ubgapPBeeIHsPl92PwhXPF3k4mgtAQO/AK7F1rmrwaYsaGWl5nsz/VUvQ0ilr3Z9wPDgCRgAzBFa727Qpm7gM5a6zuUUpOB8Vrr6yqcXwCUAeskiNjfuoRMZry7nlA/d24d0IqJPSNxdlIs2HSUt349wJHjeTT182Biz0gm9WxO80Cvc99UiLMpLYbPrzcBY9IH5Wn6T2XAe6PNOpiBD5q9ZcI7m2BTnZTt5j6n0s36mdhxZiLA1k/MGpnQGEsizMtMkKhIazOB4PdXzDqfa+aanGllpfDNHbBjPsReY7IR5KSU51sryIZiy6qI2PEw7F8mfUxZqdkv5+CvZuJB9DC7bilQn4NIP0wLYoTl9aMAWuv/VCjzk6XMH0opFyAVCNFaa6XUOOBSzDqVXAki9UNWXhE+7i5ntDZKSstYtucYn29I5Nf96WgNnSL86d86iH6tg+gTFYSnWx0MhorG5/TvqKrdpTmp8Okk0w11mm8zM/W5SUuzd0zGfnM+95jpBpvymXXZCrQ2kwpC2lce1C8tgQU3m5Q10UOh+wxoO7J8jKgwF/54HX57EVDQdrjpysvLKL+HT1OzO2fPm8+eo6wgG/YvNWNN7cacf1qc0hI4tsOsCap6TW4ayjes3gaRCcBIrfWtltfTgT5a61kVyuy0lEmyvD4A9AHygWWYVsyD1BBElFIzgZkALVq06HH48GGbvBdxYZKz8vlmy1FW7U9ny5EsikrLaBXizQ/3DMDLTWaVi1p2KhNSt0HyVrMmJuswnDhsWh3Bbc1ss/DOpjVgbbLNsykrM11hNbWCALKOmBQ0B1eZBZsxY6HV5XDot/Itn53dYcjfoc8d5YGqMBd2f2dSzxxYAWWWKdChsXDF36Dd6LMP7J84BAtuM114TTuZrrc2w82Y0ppXYd3bqMdT620QmQiMqBJEemut76lQZpelTMUg0ht4FFivtZ6vlPon0hJpsPKLSvlpVyp/+WIrtw6I4vErY+xdJeEotG44M6eyjsCiB81amYieJnPA/h9NtoCiXAhoYbITxFxtyq74t0nwGdETrn4NQjucec9tX5iZb0qZwLRjvgkq4V3M2FdhDnSagJowr94GEau7s4BVwOl2XQBmXOQfWuvXanqeBJH67bFvdvD5+iN8c9eldGluxX4ZQjR2WsOOr2DJw5B/HFy9oeN46DbDbH5WNVPAtk/NVOSiXJOtudet5txhy3YAcUuhRT8Y/7ZlBlqxSaq59k0IbmPW+oTF1usxERfMwPoQ4ChmYH2q1npXhTJ3A50qDKxfo7WeVOU+/0RaIg3eyYJihr+4igAvVxbOGoCbi8zgEqJapzLMQs6WA85M4llVzjH47i4zOaD1ENNNlbwZPAPh0vvMep9zDNrX1R7rF0xrXQLMAn4C9gDztda7lFJPKaVO56WeBwQppeKBvwKzbVUfYV9+Hq48Pa4je1NzeOvXA/aujhD1l3ewWUh5rgACJp/Z1C9h5HNmfKUgy6SguX8XDPhLncz6ksWGok7d89kWftyZwvDYpgxsE8yANiF4uzmTkVtEZm4hQT5uRIeexz8eIURlJUVmQP4CU8JIAkbRoDw1NhYvV2dW7k9j0faUM84rBbcOiOKB4e3wcJUpwUKct4q7ZNblY+3yVOGwmni78dyEzmitiUvLZU18BqUagn3cCPR248edqbyz+iC/7E3j+Yld6N7iLNMmhRB2J91Zot75LS6DRxZs52hWPh0j/BjUNoRBbUPp3iJAUqoIUcvq7eysuiZBpHHJKSjmwz8Os3JfGpuPZFFapmkb5sOTYztKSnohapEEEQsJIo1Xdn4xy/ce44Wl+0k6kc9VXZox87JW5BQUk5JdQGFJGeO6NZPV8EJYQQbWRaPn7+nK+G6RjOoYzlu/HuDNlQf4fltypTJrEzJ5ZXJXSUcvRB2TICIaDA9XZ/4ytC3Xdo9k85EThPp6EO7vwXdbk3lp2X4ujQ7iul4t/iy/LiGTlfvT6d86iL6tgnCV8RQhap0EEdHgNA/0qpRmftYV0aw/lMkTC3fRrUUT2oT6MO+3g/x78R7KNLy58gC+Hi4M7RDGgyPaERHgeZa7CyEuhIyJiEYhLaeA0a+sJtDbjZhwP77dmsyI2DCeGd+JLUeyWLorlcU7Umji7cZnt/WVvU6EsJCBdQsJImLV/nRmvLsepeCBYW25a3A0Tk7lYyTbk7KYNncdvh6ufHZbX1oEeaG1Zm9qDqnZBVzePvQsdxeicZIgYiFBRAAs2p5CEy9X+kdXv+/3zqPZTJu3Dk9XZ67pHsGSnakkpJ8C4N4rorl/WNuzDs4XlZSRllPAsZMFZOYW0ScqCH8vV5u8FyHqggQRCwki4nztTj7J9XPXkp1fTN9WQYzpHM62xCzmb0zirsGteWhEu2oDydzVCTy7ZC8lZeX/ZoJ93PjbmA6M6xohM8NEgyRTfIW4QDHN/Pj5r4MACPZxB2BKrxa4ODvxxsoDlJZpZo9qXykofLDmEE8v2sMV7UMZHhNGU38PXJ2deH7pPu7/YhvzNyTxzPiOtArxsct7EsJepCUihIXWmicW7uLDPw7TrUUAsy6P5or2oXyxIZHZX+9geEwYr1/fvdJU4bIyzWcbjvDckr0AfHJrXzpF+tvrLQhxwaQ7y0KCiKgNWms+W5/IGyvjSTqRT6sQbw5mnGJgmxDmzOiBu0v1mYUTj+cxec5acgqKJZCIBqXebkolREOklGJqnxaseHAwL07qgpuzEwPbhPD29JoDCJi1K5/P7IufpyvXz13L9qSsOqy1EPYjLREhalHSCdMiSc8ppEtkAB3CfYlp5seI2KYEeNlnvwchzka6sywkiIj6Ijkrn7d+PcCu5JPsTTnJqaJSAr3deGx0B67tbmZxFZaUsnhHCqv2ZxDZxJN2TX1p39SXVsE+lda2nJaaXYCTAk83Z7zcXHCupowQ1pAgYiFBRNRHZWWaHUezefL7XWw+kkWfqEC6X9KE+RsSyTxVRKC3G1l5RZyeNdy+qS/3D2vL8JgwlFJsPnKCF5bu4/f4zEr3DfBypamfyR3WrUUT7hjUGjcX6Z0WF06CiIUEEVGflZVpvtiYyLNL9nKyoJgh7cO4of8lXNo6mKLSMg6k57ItMZu5qxNIyDhFbDM/wvw8WL43jSBvN24eEIW/pyv5RaWcKiohM7eIlOwCkrPy2Z1yks6R/rw2pTstgrz+fN7O5GyaN/Giibd0o4maSRCxkCAiGoKcgmLyi0sJ9fWo9nxJaRnfbU3mlV/iyMor4vZBrbmxf0u83Wte0vXjzlQe/mobWsPs0e05cjyP77cmk5xdQIivO+/M6EnX5gG2ekuigZMgYiFBRDQmZZb+rerGR6qTeDyPWZ9tYVtiFs5OioFtghkaE8abKw+QnlPIi5O6MqZzOGCmMZ8qKsXnLIFJOA4JIhYSRISjKyopY21CJjHN/P5ciZ+ZW8jMjzax6fAJruwcTlpOIXtTTnKyoIRru0fyxNgY/Dwk95cjkyBiIUFEiOoVFJfy92938tOuVKJDfegQ7oeLk+KjtYcJ9/fkhUld6NtK9q13VBJELCSICHFhNh0+wQPzt3L4eB7X92nBvUPaVBqr0VpzMOMUzQO9ZFfIRkyCiIUEESEuXF5RCf/9cR8frT2Mm7MTtwyIYkRsU37alcp3246SeDyfYB93JvSI5LpezYkK9rZ3lUUtkyBiIUFECOsdyjjFiz/vZ+G2ZACcFFwaHczl7UJZcyCTFfvSKC3T9G8dxI39WzKkQ9ifCx4LikvZl5pDdKjPWWeRifpJgoiFBBEhLt6u5Gx2HT3J4PYhlbq20k4W8OWmJD5Ze5jk7AKaB3pyebtQdh7NZsfRbIpLNZ6uzgyPDWNc1wiigr1JPJFH4vF8svKLCPf3oEWgF80DvWqc3izsQ4KIhQQRIWyvpLSMpbuP8d7vB9mWmE3HCD96RQUS28yftQmZLNqeQnZ+8VnvMSwmjCfHxtIswLPS8bIyzdqETBZsPsqKfWmM7tSUJ66KlfEYG5MgYiFBRIi6pbU+YzfHwpJSVu/P4EReEc0tLY8AT1dSsvNJPJ7PlsQs5qw6gLNSPDC8HSM7NmXDoeOsTcjk133pJGcX4OvuQrdLmrBqfzoD24bw+tRu+Mo0ZJuRIGIhQUSIhiHxeB6Pf7uTX/en/3nM18OFPlFBjO3ajOExYXi4OjN/QyKPfbOD6FAfnhnfkVOFpaTlFKKAsV2bSQullkgQsZAgIkTDobVm6e5jJJ3Ip09UIB3C/arNTLw6Lp27Pt5MTmFJpePju0XwwsQu572iX9RM9lgXQjQ4SilGxDY9Z7nL2oSw+L7L2HE0mxBfd0J83Fm4LZkXf96Pn4cL/xwbe0aXmqhbEkSEEPXa6bGV0+65IpqcgmLeWX0Qf09X/jK0LSknCziccYpQPw+iQ30qXa+15vvtKcQ286N1iE/V24uLZNMgopQaCbwCOANztdbPVjnvDnwI9AAygeu01oeUUsOAZwE3oAh4SGu93JZ1FUI0DEopHhvdgZP5Jby6PJ63fk2gqLQMAFdnxauTuzGqU3myyRd/3s//lscTFezNkvsuw8O1fJvj+LRcnl60m4dGtCO2mX+l56xLyGTNgUzuurz1WbdGdnQ2CyJKKWfgdWAYkARsUEot1FrvrlDsFuCE1jpaKTUZeA64DsgArtJaJyulOgI/ARG2qqsQomFRSvHvazrRIsiLkwXFXBLoTfNAT15eFsfdn27m+YlduKZ7JC8ti+N/y+MZEB3Mb/EZvPJLHI+MbA9AflEpd3+ymX3HctibksN3sy4lzM+sYdmelMVN728gr6iUtQmZzJneE38vmSFWHVtOb+gNxGutE7TWRcDnwNVVylwNfGD5+StgiFJKaa23aK2TLcd3AR6WVosQQgDg7KS4+/JoHh3Vgal9WnBZmxA+uqU3/VoH8df527jl/Q28+ksck3pG8uHNvZnUM5I5qxLYeTQbgKd+2MW+Yzn8bXQHThYUc+sHG8krKuFIZh43v7+BJl5uPDk2li1Hsrjmzd9JPJ5n53dcP9myOysCSKzwOgnoU1MZrXWJUiobCMK0RE67FtiitS6s+gCl1ExgJkCLFi1qr+ZCiAbJy82FeTf0Ytanm1m2J42JPSJ59prOODkp/jY6hhX70nlkwXZuGRDFZ+sTuXNwa24b2IqoYG9u+2gj9362hQPppygp03x+c2+iQ31o39SXmR9t4urXf6dfqyBCfN0J9XNnaIcw2ob52vst253NpvgqpSYCI7TWt1peTwd6a63vqVBml6VMkuX1AUuZTMvrWGAhMFxrfeBsz5MpvkKI04pLy9h46AS9owIrTR3+cWcKd3y8GYCelzTh85l9cbGsN5m7OoGnF+3B3cWJT2/rQ49LAv+87kB6Lk//sJvDx/NIP1lITmEJHq5OvDipK6Mt4y/VOVlQzPbEbPYdy2F/ag7Z+cUMaBPMkA6hhPt71nhdXarPU3yTgOYVXkcCyTWUSVJKuQD+wHEApVQk8A0w41wBRAghKnJ1dqJf6zP3SBnZMZwrO4fzx4FMXp3S7c8AAnDLgCjcXJyIDvGpFEAAWof48N5Nvf98fexkAXd+vIm7PtnM/UPbcu+Q6EpTjUtKy/h0/RGe/2kfJwvMGpdAbzc8XZ35cVcqj38LHSP8uLxdKIPahtC1eUClujQktmyJuAD7gSHAUWADMFVrvatCmbuBTlrrOywD69dorScppQKAX4GntNYLzud50hIRQpyPsjJNfnHpRWccLigu5bGvd/D1lqMMbBtC31aBtAzyxs3ZiRd+3s+elJNcGh3EHYNa0yHc7DaptSY+LZdle9L4Zc8xNh85QZkGPw8Xhsc25dbLomjf1O/PZ8Sn5bJkRwpXdmlmszT89XrFulJqNPAyZorvu1rrZ5RSTwEbtdYLlVIewEdAN0wLZLLWOkEp9TjwKBBX4XbDtdZpNT1LgogQoq5prZmzKoF3Vh8kI7d82LaZvwePXxnDqI5Nz7oYMjuvmN/iM1ixL41F21PILy5lUNsQBrUNYfGOFDYePgFAUz8PvryjX6X1MrWhoLgUTzeX+htE6pIEESGEPeUUFHM4M4+0nAL6tgrCy+3CWjonThXxybrDvL/mMBm5hbQK8WZyr+Z0bObPHR9voom3G1/e3o9Qv9pJpZ9bWMJtH2zk89v7SRABCSJCiMahoLiUpBN5tA7x+bMVs+XICa6fu47mTbyYM6MHWXnFHDmeR1FJGVd2Cb/gxZDHTxVx43vr2ZV8koT/jJEgAhJEhBCN25oDGdz43gaKSsoqHW/f1JcXJ3UlppkZSykoLuXHnalk5BbSv3UwHcJ9K3WppWYXMH3eOo4cz+P1qd0ZFttUgghIEBFCNH7bErPYdPiEJZ+YJ0cy8/jbtzvJyivi7sujyS8qZf7GRE7klW8MFuzjRrcWTcgtKCEtp4DkrAKcnRRzb+hJ31ZB9XtgvS5JEBFCOKITp4p4/LudLNqegrOTYnhMGNP6XkKrEG9+i8vgt/gMdiWfJMDTlVA/d0J9PbiuV3M6hJuWiwQRCwkiQghHtj0pi1BfD5r6X9jAe31ebCiEEKKOdI4MsMtzG+YSSSGEEPWCBBEhhBBWkyAihBDCahJEhBBCWE2CiBBCCKtJEBFCCGE1CSJCCCGsJkFECCGE1SSICCGEsJoEESGEEFaTICKEEMJqEkSEEEJYTYKIEEIIq0kQEUIIYTUJIkIIIawmQUQIIYTVJIgIIYSwmgQRIYQQVpMgIoQQwmoSRIQQQlhNgogQQgirSRARQghhNQkiQgghrCZBRAghhNUkiAghhLCaBBEhhBBWkyAihBDCahJEhBBCWE2CiBBCCKvZNIgopUYqpfYppeKVUrOrOe+ulPrCcn6dUqplhXOPWo7vU0qNsGU9hRBCWMdmQUQp5Qy8DowCYoApSqmYKsVuAU5oraOBl4DnLNfGAJOBWGAk8IblfkIIIeoRW7ZEegPxWusErXUR8DlwdZUyVwMfWH7+ChiilFKW459rrQu11geBeMv9hBBC1CMuNrx3BJBY4XUS0KemMlrrEqVUNhBkOb62yrURVR+glJoJzLS8LFRK7aydqjd4wUCGvStRT8hnUU4+i3LyWZRrdzEX2zKIqGqO6fMscz7XorWeA8wBUEpt1Fr3vNBKNkbyWZSTz6KcfBbl5LMop5TaeDHX27I7KwloXuF1JJBcUxmllAvgDxw/z2uFEELYmS2DyAagjVIqSinlhhkoX1ilzELgBsvPE4DlWmttOT7ZMnsrCmgDrLdhXYUQQljBZt1ZljGOWcBPgDPwrtZ6l1LqKWCj1nohMA/4SCkVj2mBTLZcu0spNR/YDZQAd2utS8/xyDm2ei8NkHwW5eSzKCefRTn5LMpd1GehzH/8hRBCiAsnK9aFEEJYTYKIEEIIqzWKIHKu9CqNmVKquVJqhVJqj1Jql1LqPsvxQKXUz0qpOMvfTexd17qglHJWSm1RSv1geR1lSakTZ0mx42bvOtYVpVSAUuorpdRey/ejnwN/L+63/PvYqZT6TCnl4SjfDaXUu0qptIrr6Gr6HijjVcvv0u1Kqe7nun+DDyLnmV6lMSsBHtBadwD6Andb3v9s4BetdRvgF8trR3AfsKfC6+eAlyyfwwlMqh1H8Qrwo9a6PdAF87k43PdCKRUB3Av01Fp3xEz0mYzjfDfex6SPqqim78EozGzYNpiF3G+e6+YNPohwfulVGi2tdYrWerPl5xzML4oIKqeU+QAYZ58a1h2lVCQwBphrea2AKzApdcBBPgcApZQfMBAzAxKtdZHWOgsH/F5YuACelvVoXkAKDvLd0Fqvwsx+raim78HVwIfaWAsEKKXCz3b/xhBEqkuvckaKFEdgyYLcDVgHhGmtU8AEGiDUfjWrMy8DDwNlltdBQJbWusTy2pG+G62AdOA9S/feXKWUNw74vdBaHwWeB45ggkc2sAnH/W5Azd+DC/592hiCyHmlSGnslFI+wALgL1rrk/auT11TSl0JpGmtN1U8XE1RR/luuADdgTe11t2AUzhA11V1LP39VwNRQDPAG9NtU5WjfDfO5oL/zTSGIOLwKVKUUq6YAPKJ1vpry+Fjp5uhlr/T7FW/OnIpMFYpdQjTpXkFpmUSYOnCAMf6biQBSVrrdZbXX2GCiqN9LwCGAge11ula62Lga6A/jvvdgJq/Bxf8+7QxBJHzSa/SaFn6/ecBe7TWL1Y4VTGlzA3Ad3Vdt7qktX5Uax2ptW6J+Q4s11pfD6zApNQBB/gcTtNapwKJSqnTGVqHYDJAONT3wuII0Fcp5WX593L6s3DI74ZFTd+DhcAMyyytvkD26W6vmjSKFetKqdGY/3WeTq/yjJ2rVGeUUgOA1cAOyscCHsOMi8wHWmD+EU3UWlcdXGuUlFKDgQe11lcqpVphWiaBwBZgmta60J71qytKqa6YSQZuQAJwE+Y/jg73vVBKPQlch5nNuAW4FdPX3+i/G0qpz4DBmPT3x4AngG+p5ntgCbKvYWZz5QE3aa3PmuW3UQQRIYQQ9tEYurOEEELYiQQRIYQQVpMgIoQQwmoSRIQQQlhNgogQQgirSRAR4hyUUqVKqa0V/tTaym+lVMuK2VWFaGhstj2uEI1Ivta6q70rIUR9JC0RIayklDqklHpOKbXe8ifacvwSpdQvlv0YflFKtbAcD1NKfaOU2mb5099yK2el1DuW/S6WKqU8LeXvVUrtttznczu9TSHOSoKIEOfmWaU767oK505qrXtjVvm+bDn2GiaddmfgE+BVy/FXgV+11l0weax2WY63AV7XWscCWcC1luOzgW6W+9xhqzcnxMWQFetCnINSKldr7VPN8UPAFVrrBEsSzFStdZBSKgMI11oXW46naK2DlVLpQGTF1BqW9P0/WzYHQin1COCqtX5aKfUjkItJUfGt1jrXxm9ViAsmLREhLo6u4eeaylSnYr6mUsrHKsdgdu3sAWyqkHFWiHpDgogQF+e6Cn//Yfl5DSaTMMD1wG+Wn38B7oQ/94L3q+mmSiknoLnWegVmo60A4IzWkBD2Jv+zEeLcPJVSWyu8/lFrfXqar7tSah3mP2RTLMfuBd5VSj2E2V3wJsvx+4A5SqlbMC2OOzE77VXHGfhYKeWP2SjoJcv2tkLUKzImIoSVLGMiPbXWGfauixD2It1ZQgghrCYtESGEEFaTlogQQgirSRARQghhNQkiQgghrCZBRAghhNUkiAghhLDa/wP7U8s16QlpaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015733333333333332\n",
      "0.0379\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(nepochs+1), epoch_error_train, label=\"train\")\n",
    "plt.plot(range(nepochs+1), epoch_error_test, label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = nepochs\n",
    "plt.axis([0,xmax,0.0,0.1])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.min(epoch_error_train))\n",
    "print(np.min(epoch_error_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
