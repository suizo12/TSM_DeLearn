{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the exercise is to implement computational graphs in TensorFlow to train and use such an architecture. The constraints we put ourselves is to use **low-level** functions of TensorFlow, i.e. we will not use high-level functions to compose layers and to train the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Dataset Preparation #\n",
    "#######################\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_vec),(x_test, y_test_vec) = mnist.load_data()\n",
    "x_train, x_test = (x_train / 255.0) - 0.5, (x_test / 255.0) - 0.5\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "# convert class vectors to binary class matrices\n",
    "n_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train_vec, n_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_vec, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample a random batch from dataset\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels[idx]\n",
    "\n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Training phase #\n",
    "##################\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "E = 50               # number of epochs\n",
    "B = 128               # batch size\n",
    "N = x_train.shape[0]  # number of samples\n",
    "D = x_train.shape[1]  # dimension of input sample\n",
    "H = 300               # number of neurons\n",
    "A = 0.01              # learning rate alpha\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "\n",
    "# Build the computational graph\n",
    "\n",
    "# define placeholders x, y and learning rate alpha\n",
    "# x = ...\n",
    "# y = ...\n",
    "# alpha = ...\n",
    "# define TensorFlow Variables for w1, b1, w2, b2 following the given examples\n",
    "w1 = tf.Variable(tf.truncated_normal((D, H), stddev = 0.1))\n",
    "b1 = tf.Variable(tf.constant(0.0, shape=[H]))\n",
    "# w2 = ...\n",
    "# b2 = ...\n",
    "\n",
    "# define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "# h = ...\n",
    "# y_pred = ...\n",
    "# define nodes for difference between predicted and target values and for loss\n",
    "# diff = ...\n",
    "# loss = ...\n",
    "# define the gradients\n",
    "# grad_w1, grad_b1, grad_w2, grad_b2 = ...\n",
    "\n",
    "# compute the new values of the gradients with the Variable assign method (see slides)\n",
    "# new_w1 = ...\n",
    "# new_b1 = ...\n",
    "# new_w2 = ...\n",
    "# new_b2 = ...\n",
    "updates = tf.group(new_w1, new_b1, new_w2, new_b2)\n",
    "\n",
    "# Run the computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    J = []\n",
    "    for epoch in range(E):\n",
    "        for _ in range(int(N/B)): # number of batches to visit for 1 epoch\n",
    "            # get batches calling the next_batch method provided above\n",
    "            # x_train_batch, y_train_batch = ...\n",
    "            # define a dictionary of values that will be used to feed the placeholders of the graph\n",
    "            # values = { ... }\n",
    "            # ask TensorFlow to compute the graph on the batch and update the values\n",
    "            # loss_val = sess.run(...)\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val[0])\n",
    "\n",
    "    # now retrieve the weights and bias out of the computational graph\n",
    "    w1_trained, b1_trained, w2_trained, b2_trained = sess.run([w1, b1, w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss\n",
    "pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Testing phase #\n",
    "#################\n",
    "\n",
    "N = x_test.shape[0]  # number of samples\n",
    "D = x_test.shape[1]  # dimension of input sample\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "# Build the computational graph\n",
    "# x = ...\n",
    "# y = ...\n",
    "# w1 = ...\n",
    "# b1 = ...\n",
    "# w2 = ...\n",
    "# b2 = ...\n",
    "\n",
    "# define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "# h = ...\n",
    "# y_pred = ...\n",
    "\n",
    "# Run the computational graph\n",
    "with tf.Session() as sess:\n",
    "    # define a dictionary of values that will be used to feed the placeholders of the graph\n",
    "    # don't forget to pass in the trained weights and biases\n",
    "    # values = ...\n",
    "    # ask TensorFlow to compute the graph on the test set\n",
    "    # values = { ... }\n",
    "    # y_pred_test = sess.run(...)\n",
    "\n",
    "# At this stage, y_pred_test should contain the matrix of outputs on the test set with shape (N_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "y_winner = np.argmax(y_pred_test, axis=1)\n",
    "N_test = y_winner.size\n",
    "num_correct = (y_winner == y_test_vec).sum()\n",
    "num_missed = N_test - num_correct\n",
    "accuracy = num_correct * 1.0 / N_test\n",
    "error_rate = num_missed * 1.0 / N_test\n",
    "print('# correct  : ', num_correct)\n",
    "print('# missed   : ', num_missed)\n",
    "print('accuracy   :  %2.2f %%'% (accuracy*100.0))\n",
    "print('error rate :  %2.2f %%'% (error_rate*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
