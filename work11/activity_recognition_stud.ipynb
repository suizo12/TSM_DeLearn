{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "The data is taken from http://archive.ics.uci.edu/ml/machine-learning-databases/00240/, but you can have it in the form of the UCI-HAR-Dataset.zip.\n",
    "\n",
    "Extract it to the folder of your notebook with the solution (or suitably adjust the paths to training and test data specified below (TRAIN, TEST).  \n",
    "\n",
    "Spcifically, the split into training and testing data is already done. The dataset contains 7352 training and 2947 test samples.  \n",
    "\n",
    "Each sample consists of 9 timeseries of float values, each of length 128.\n",
    "\n",
    "The 9 timeseries are collected by an accelerometer and a gyroscope that contain information about the motion and the orientation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "TRAIN = \"UCI-HAR-Dataset/train/\"\n",
    "TEST = \"UCI-HAR-Dataset/test/\"\n",
    "\n",
    "# x-values, 'signals'\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "X_train_signals_paths = [TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES]\n",
    "X_test_signals_paths =  [TEST  + \"Inertial Signals/\" + signal + \"test.txt\"  for signal in INPUT_SIGNAL_TYPES]\n",
    "\n",
    "\n",
    "# y-values, 'labels'\n",
    "#Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "N_CLASSES = len(LABELS)\n",
    "y_train_path = TRAIN + \"y_train.txt\"\n",
    "y_test_path  = TEST  + \"y_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ - 1\n",
    "\n",
    "def one_hot(y, n_classes):\n",
    "    '''\n",
    "    For the given label y (0<=y<n_classes) create a one-hot vector of lenght n_classes.\n",
    "    '''\n",
    "    ### START YOUR CODE \n",
    "\n",
    "    \n",
    "    ### END YOUR CODE \n",
    "\n",
    "def label_from_onehot(vectors):\n",
    "    '''\n",
    "    For given list of vectors of scores, create a list of (predicted) labels.\n",
    "    '''\n",
    "    ### START YOUR CODE \n",
    "\n",
    "    \n",
    "    ### END YOUR CODE \n",
    "\n",
    "Y_train = one_hot(load_y(y_train_path), N_CLASSES)\n",
    "Y_test = one_hot(load_y(y_test_path), N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "labels = np.array(range(N_CLASSES))\n",
    "np.testing.assert_equal(labels,label_from_onehot(one_hot(labels, N_CLASSES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_per_class = {}\n",
    "for c in range(N_CLASSES):\n",
    "    mask = label_from_onehot(Y_train)==c\n",
    "    X_train_per_class[c] = X_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(X, nsamples):\n",
    "    t = range(128)\n",
    "    for i in range(9):\n",
    "        plt.subplot(3,3,i+1)\n",
    "        for j in range(np.minimum(X.shape[0],nsamples)):\n",
    "            plt.plot(t,X[j,:,i])\n",
    "    plt.show()\n",
    "    \n",
    "plot_lines(X_train_per_class[5], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "\n",
    "print(\"(X shape, Y_shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, Y_train.shape, np.mean(X_train), np.std(X_train))\n",
    "print(X_test.shape, Y_test.shape, np.mean(X_test), np.std(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import LSTM, RNN, SimpleRNN, Dropout\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer RNN\n",
    "\n",
    "Specify a model based on a <code>SimpleRNN</code> (<code>keras.layers.SimpleRNN</code>) followed by a softmax for the classification.\n",
    "\n",
    "Try different number of units. Do hyper-parameter tuning and report the best test accuracy you can achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile model \n",
    "### START YOUR CODE \n",
    "n_steps = \n",
    "n_input = \n",
    "\n",
    "n_hidden = \n",
    "n_classes = \n",
    "\n",
    "...\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(...)\n",
    "model.add(Dense(..., activation='softmax'))\n",
    "model.compile(...)\n",
    "\n",
    "### END YOUR CODE \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "batch_size = \n",
    "n_epochs = \n",
    "log = model.fit( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['loss'], label='Training')\n",
    "plt.plot(log.history['val_loss'], label='Testing')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(log.history['acc'], label='Training')\n",
    "plt.plot(log.history['val_acc'], label='Testing')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "### START YOUR CODE \n",
    "\n",
    "\n",
    "### END YOUR CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Regularisation\n",
    "\n",
    "Implement L2-Regularisation for <code>SimpleRNN</code> (single layer) and the Softmax Layer. Investigate how you can do that in <code>keras</code>.\n",
    "\n",
    "Implement Gradient Clipping. Find out how you can do that in <code>keras</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model incl. L2 regularisation\n",
    "\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient clipping and train the model.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results by computing the accuracy and the confusion matrix.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Layers\n",
    "\n",
    "Specify a model based on stacked <code>SimpleRNN</code>'s (<code>keras.layers.SimpleRNN</code>) and a Softmax layer for the final classification. \n",
    "\n",
    "Again try different number of hidden units, do hyper-parameter tuning and report the best test accuracy you can achieve with two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = \n",
    "n_input = \n",
    "n_hidden1 = \n",
    "...\n",
    "n_classes = \n",
    "### START YOUR CODE \n",
    "\n",
    "model = ...\n",
    "\n",
    "\n",
    "### END YOUR CODE \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = \n",
    "n_epochs = \n",
    "log = model.fit( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['loss'], label='Training')\n",
    "plt.plot(log.history['val_loss'], label='Testing')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log.history['acc'], label='Training')\n",
    "plt.plot(log.history['val_acc'], label='Testing')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results by computing the accuracy and the confusion matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
