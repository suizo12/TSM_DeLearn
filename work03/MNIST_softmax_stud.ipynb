{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Full classification of MNIST data.\n",
    "\n",
    "The original MNIST dataset is used.\n",
    "\n",
    "In the following, we use the following notation (see also the notations sheet):\n",
    "\n",
    "m: Number of samples <br>\n",
    "n: Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (70000, 784)\n",
      "Label Data Shape (70000,)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  22 183 252 254 252 252\n",
      "  252  76   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  85  85 168 250 250 252 250 250\n",
      "  250 250  71   0  43  85  14   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 107 252 250 250 250 250 252 250 250\n",
      "  250 250 210   0 127 250 146   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 114 237 252 250 250 250 250 252 250 250\n",
      "  250 250 210   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0   0   0 107 237 250 252 250 250 250  74  41  41  41\n",
      "   41 217  34   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0   0  15 148 252 252 254 238 105   0   0   0   0   0\n",
      "    0   0   0   0 128 252 252   0   0   0]\n",
      " [  0   0   0   0   0  15 140 250 250 250 167 111   0   0   0   0   0   0\n",
      "    0   0   0   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0  43 250 250 250 250   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0 183 250 250 250 110   0   0   0   0   0   0   0   0\n",
      "    0   0   0  57 210 250 250   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 110   7   0   0   0   0   0   0   0   0\n",
      "    0   0   0  85 250 250 250   0   0   0]\n",
      " [  0   0   0   0   0 254 252 252  83   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  86 252 252 217   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 138  14   0   0   0   0   0   0   0   0\n",
      "    0   0  15 140 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 250  41   0   0   0   0   0   0   0   0\n",
      "    0   0  43 250 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 250 181   0   0   0   0   0   0   0   0\n",
      "    0   0 183 250 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0  76 250 250 250 250   0   0   0   0   0   0   0   0\n",
      "    0 177 252 250 250 110   7   0   0   0]\n",
      " [  0   0   0   0   0  36 224 252 252 252 219  43  43  43   7   0  15  43\n",
      "  183 252 255 252 126   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 250 250 250 252 250 250 250 111  86 140 250\n",
      "  250 250 252 222  83   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  42 188 250 250 252 250 250 250 250 252 250 250\n",
      "  250 250 126  83   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 127 250 250 252 250 250 250 250 252 250 250\n",
      "  137  83   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  21  41 217 252 250 250 250 250 217  41  41\n",
      "   14   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "x, y = mnist['data'], np.array(mnist['target'], dtype='int')\n",
    "\n",
    "print(\"Image Data Shape\" , x.shape)\n",
    "print(\"Label Data Shape\", y.shape)\n",
    "\n",
    "image = x[17,:]\n",
    "plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "print(np.reshape(image, (28,28)))\n",
    "print(y[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use 10'000 test samples (~14%).\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training set:  (784, 60000) (1, 60000)\n",
      "Shape test set:      (784, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=0)\n",
    "\n",
    "# reshape: \n",
    "# for x a simple transpose is sufficient \n",
    "# (m,n) -> (n,m) where m is the number of samples and n the number of input features (pixels)\n",
    "# for y reshape the simple array to become a (1,m) array\n",
    "x_train1 = x_train0.T\n",
    "x_test1 = x_test0.T\n",
    "m_train = x_train0.shape[0]\n",
    "m_test = x_test0.shape[0]\n",
    "y_train=y_train.reshape(1,m_train)\n",
    "y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "print(\"Shape training set: \", x_train1.shape, y_train.shape)\n",
    "print(\"Shape test set:     \", x_test1.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centering later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xmax = np.max(x_train1)\n",
    "xmin = np.min(x_train1)\n",
    "print(xmin, xmax)\n",
    "x_train = x_train1 / xmax\n",
    "x_test = x_test1 / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Compute the per class probabilities for all the m samples by using a softmax layer with parameters (W, b).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array with shape (ny, nx) (with ny=10 for MNIST).\n",
    "    b -- biases, a numpy array with shape (ny,1)\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    \n",
    "    Returns:\n",
    "    A -- a numpy array of shape (ny,m) or (ny,1) with the prediction probabilities for the digits.\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    A = softmax(np.dot(X.T, W.T)+b.T)\n",
    "    ### END YOUR CODE ###\n",
    "    return A.T\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([2, 3]).reshape(2,1)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([0.01587624,0.86681333,0.11731043]).reshape(A.shape)\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), 1.0, decimal=8)\n",
    "\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([[0.46831053, 0.01321289, 0.21194156, 0.01321289],\n",
    " [0.46831053, 0.26538793, 0.57611688, 0.26538793],\n",
    " [0.06337894, 0.72139918, 0.21194156, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Cross Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapey(yhat,y):\n",
    "    \"\"\"\n",
    "    Checks whether the inputs come as a list in which case it reshapes it to (1,m).\n",
    "    Implementation is sloppy...\n",
    "    \"\"\"\n",
    "    if type(yhat).__module__ == np.__name__:\n",
    "        m = yhat.size\n",
    "        yhat = yhat.reshape(1,m)\n",
    "        y = y.reshape(1,m)\n",
    "    else:\n",
    "        m = 1\n",
    "    return yhat, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    Ypred -- prediction from softmax, a numpy array of shape (ny,m) or (ny,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    m = len(Y)\n",
    "    \n",
    "    # see cost function on slide 13\n",
    "    #Select the right index.\n",
    "    # The idea is, Y represents the right label, for the secon example the labels are 1, 1, 1, 0\n",
    "    # ypred is (rounded 2 decimals) [[0.02 0.01 0.05 1.  ]\n",
    "    #                                [0.98 0.99 0.95 0.  ]]\n",
    "    # which means that for the first h the % that it is a zero is 2% and that it is a 1 is 98%\n",
    "    # now you select the value from your Y value meaning 0.98, 0.99, 0.95, 1  with Ypred.T[np.arange(m), Y]\n",
    "    ### END YOUR CODE ###\n",
    "    return - (np.log(Ypred.T[np.arange(m), Y]).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Cross Entropy Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second test\n",
      "[[0.02 0.01 0.05 1.  ]\n",
      " [0.98 0.99 0.95 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1])\n",
    "Ypred = np.array([0.04742587,0.95257413]).reshape(2,1)\n",
    "J = cost(Ypred,Y)\n",
    "Jexp = 0.04858735\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)\n",
    "print('second test')\n",
    "Y = np.array([1,1,1,0])\n",
    "Ypred = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]])\n",
    "print(np.round(Ypred, 2))\n",
    "Jexp = 0.01859102\n",
    "J = cost(Ypred,Y)\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y,n):\n",
    "    \"\"\"\n",
    "    Constructs a one-hot-vector from a given array of labels (shape (1,m)) and the number of classes n.\n",
    "    The resulting array has shape (n,m) and in row j and column i a '1' if the i-th sample had a label 'j'.\n",
    "    We assume that the labels can have values (0,1,2,...,n-1). \n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "    result = np.zeros((n,m),dtype=float)\n",
    "    result[y[0,:],np.arange(m)] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m).\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. W (key \"dW\" with shape (ny,nx)) and w.r.t. b (key \"db\" with shape (ny,1))\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###   \n",
    "    m = Y.shape[1]\n",
    "    n = W.shape[0]\n",
    "    \n",
    "    yhat = predict(W, b, X)\n",
    "    one_hot = onehot(Y, n)\n",
    "    \n",
    "    gradJ = {}\n",
    "    gradJ['dW'] = -1/m * np.dot((one_hot - yhat), X.T)\n",
    "    gradJ['db'] = -1/m * np.sum((one_hot - yhat), axis=1).reshape(b.shape)\n",
    "    ### END YOUR CODE ###\n",
    "    return gradJ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Calculation of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "Y = np.array([1,1,1,1]).reshape(1,4)\n",
    "gradJ = gradient(W,b,X,Y)\n",
    "dW = gradJ['dW']\n",
    "db = gradJ['db']\n",
    "dWexp = np.array([[ 0.28053421,0.17666947],\n",
    "                  [-0.00450948,-0.60619918],\n",
    "                  [-0.27602473,0.42952972]]).reshape(3,2)\n",
    "dbexp = np.array([0.17666947,-0.60619918,0.42952972]).reshape(3,1)\n",
    "np.testing.assert_array_almost_equal(dW,dWexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(db,dbexp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Output Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    Ypredargmax = np.argmax(Ypred, axis=0)\n",
    "    return np.sum(Y != Ypredargmax) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PIXELS = (28,28)\n",
    "COLS = 5\n",
    "def plot_digits(X,Y,indices):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with up to 8 columns\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (1, 64)\n",
    "    Y -- label (a scalar)\n",
    "    indices -- list of indices    \n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        print(\"No images to show!\")\n",
    "        return\n",
    "    cols = min(COLS, len(indices))\n",
    "    rows = len(indices)/COLS+1\n",
    "    plt.figure(figsize=(20,5*rows))\n",
    "    for index, (image, label) in enumerate(zip(X.T[indices,:], Y.T[indices,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, PIXELS), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (indices[index],label), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(nx,ny, random=False):\n",
    "    \"\"\"\n",
    "    This function creates initial values:\n",
    "    * for w a vector of zeros of shape (1,n) [random=False] or a vector of normally distributed random values [random=True] \n",
    "    * for b set to 0.\n",
    "    \n",
    "    Argument:\n",
    "    nx,ny -- number of input features and number of outputs\n",
    "\n",
    "    Returns:\n",
    "    W -- initialized weights matrix of shape (ny,nx)\n",
    "    b -- initialized bias array of shape (ny,1)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        W = np.random.randn(*(ny,nx))\n",
    "    else:\n",
    "        W = np.zeros((ny,nx))\n",
    "    b = np.zeros((ny,1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(W, b, x_train, y_train, x_test, y_test, nepochs, alpha):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    Returns the trained parameters values as dictionary (keys \"w\" and \"b\") and various quantities \n",
    "    collected during learning also as dictionary: cost on train and test set (\"cost_train\", \"cost_test\"), \n",
    "    error rate on train and test set (\"error_train\", \"error_test\"), learning speed as length of dw and db\n",
    "    multiplied by alpha with key \"step_w\" and \"step_b\", respectively.\n",
    "    The output is provided in form of dictionaries (basically, to avoid handling too many variable names in \n",
    "    functional calls).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\"     \n",
    "    # The following lists are used for tracking the learning progress so that learning curves can be plotted.\n",
    "    # Append an according value in each epoch\n",
    "    epochs = []  # fill here the epoch id (the iteration index when looping over nepochs)\n",
    "    train_costs = [] # fill here the cost on the training set  \n",
    "    test_costs = [] # fill here the cost on the test set \n",
    "    train_errors = [] # fill here the error rate on the training set\n",
    "    test_errors = [] # fill here the error rate on the test set \n",
    "    stepsize_w = [] # fill here the lenght of the gradient of the weights vector multiplied with alpha (for the training set)\n",
    "    stepsize_b = [] # fill here the absolute value of derivative wr.t. the bias multiplied with alpha (for the training set)\n",
    "        \n",
    "        \n",
    "    for i in range(nepochs):\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        # TRAIN\n",
    "        learning_step = gradient(W, b, x_train, y_train)\n",
    "        \n",
    "        dW = alpha * learning_step['dW']\n",
    "        db = alpha * learning_step['db']   \n",
    "        W = W - dW\n",
    "        b = b - db\n",
    "        \n",
    "        # Store curve data\n",
    "        yhat = predict(W, b, x_train)\n",
    "        epochs.append(i)\n",
    "        stepsize_w.append(np.linalg.norm(dW))\n",
    "        stepsize_b.append(np.linalg.norm(db))\n",
    "        train_costs.append(cost(yhat, y_train))\n",
    "        train_error = error_rate(yhat, y_train)\n",
    "        train_errors.append(train_error)\n",
    "\n",
    "        # TEST\n",
    "        yhat = predict(W, b, x_test)\n",
    "        test_costs.append(cost(yhat, y_test))\n",
    "        test_error = error_rate(yhat, y_test)\n",
    "        test_errors.append(test_error)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)    \n",
    "    \n",
    "    params = {\"w\": W, \"b\": b}    \n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "        \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_mbgd(W, b, x_train, y_train, x_test, y_test, nepochs, batchsize, alpha, smooth=False):\n",
    "    \"\"\"\n",
    "    Same as the optimize function above - except that it should implement batch gradient descent with given batchsize.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    batchsize -- size of mini-batch\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    smooth -- if true the measures for the learning curves are smoothed per epoch\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\" \n",
    "    epochs = []\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    stepsize_w = []\n",
    "    stepsize_b = []\n",
    "        \n",
    "    n = x_train.shape[0] # number of inputs\n",
    "    m = x_train.shape[1] # number of samples\n",
    "    mb = int(m/batchsize)\n",
    "    \n",
    "    indices_train = np.arange(x_train.shape[1])\n",
    "    indices_test = np.arange(x_test.shape[1])\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "# Randomize inputs\n",
    "        np.random.shuffle(indices_train)\n",
    "        np.random.shuffle(indices_test)\n",
    "        x_train_epoch = x_train[:, indices_train]\n",
    "        y_train_epoch = y_train[:, indices_train]\n",
    "        x_test_epoch = x_test[:, indices_test]\n",
    "        y_test_epoch = y_test[:, indices_test]\n",
    "        \n",
    "        dW_steps = 0\n",
    "        db_steps = 0\n",
    "        for step in range(mb):\n",
    "            start = step * batchsize\n",
    "            end = step * batchsize + batchsize\n",
    "            \n",
    "            x_train_mini = x_train_epoch[:, start : end]\n",
    "            y_train_mini = y_train_epoch[:, start : end]\n",
    "            \n",
    "            if y_train_mini.shape[1] == 0:  # add other convergence criterias here\n",
    "                # no more training data left -> continue with next epoch\n",
    "                continue \n",
    "            \n",
    "            # predict for mini batch\n",
    "            y_train_mini_hat = predict(W, b, x_train_mini)\n",
    "            learning_step = gradient(W, b, x_train_mini, y_train_mini)\n",
    "            dW = alpha * learning_step['dW']\n",
    "            db = alpha * learning_step['db']   \n",
    "            W = W - dW\n",
    "            b = b - db\n",
    "            \n",
    "            dW_steps += dW\n",
    "            db_steps += db\n",
    "\n",
    "        yhat = predict(W, b, x_train)\n",
    "        epochs.append(i)\n",
    "        stepsize_w.append(np.linalg.norm(dW_steps))\n",
    "        stepsize_b.append(np.linalg.norm(db_steps))\n",
    "        train_costs.append(cost(yhat, y_train))\n",
    "        train_error = error_rate(yhat, y_train)\n",
    "        train_errors.append(train_error)\n",
    "\n",
    "        # TEST\n",
    "        yhat = predict(W, b, x_test)\n",
    "        test_costs.append(cost(yhat, y_test))\n",
    "        test_error = error_rate(yhat, y_test)\n",
    "        test_errors.append(test_error)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)\n",
    "        \n",
    "    params = {\"w\": W, \"b\": b}\n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "    \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.1 0.1023\n",
      "2 0.08973333333333333 0.0913\n",
      "3 0.08651666666666667 0.089\n",
      "4 0.08373333333333334 0.0883\n",
      "5 0.08168333333333333 0.0859\n",
      "6 0.08015 0.0839\n",
      "7 0.07955 0.0847\n",
      "8 0.0779 0.0828\n",
      "9 0.07723333333333333 0.0818\n",
      "10 0.07655 0.0817\n",
      "11 0.07481666666666667 0.08\n",
      "12 0.07521666666666667 0.0813\n",
      "13 0.07423333333333333 0.0804\n",
      "14 0.0745 0.0821\n",
      "15 0.07426666666666666 0.0808\n",
      "16 0.07363333333333333 0.0803\n",
      "17 0.07306666666666667 0.0792\n",
      "18 0.07218333333333334 0.081\n",
      "19 0.07178333333333334 0.0799\n",
      "20 0.07135 0.0802\n",
      "21 0.07156666666666667 0.0795\n",
      "22 0.07111666666666666 0.0786\n",
      "23 0.07128333333333334 0.0802\n",
      "24 0.07065 0.0793\n",
      "25 0.07038333333333334 0.0792\n",
      "26 0.0706 0.0798\n",
      "27 0.0709 0.0797\n",
      "28 0.07033333333333333 0.0786\n",
      "29 0.07121666666666666 0.0807\n",
      "30 0.06988333333333334 0.0795\n",
      "31 0.06996666666666666 0.0789\n",
      "32 0.06945 0.0784\n",
      "33 0.06916666666666667 0.0791\n",
      "34 0.06878333333333334 0.0779\n",
      "35 0.06938333333333334 0.0779\n",
      "36 0.06986666666666666 0.0798\n",
      "37 0.06881666666666666 0.0773\n",
      "38 0.06946666666666666 0.0796\n",
      "39 0.06863333333333334 0.0778\n",
      "40 0.07018333333333333 0.08\n",
      "41 0.06823333333333333 0.0783\n",
      "42 0.06791666666666667 0.0777\n",
      "43 0.06791666666666667 0.0767\n",
      "44 0.06751666666666667 0.0779\n",
      "45 0.0682 0.0786\n",
      "46 0.06786666666666667 0.0787\n",
      "47 0.068 0.0794\n",
      "48 0.06815 0.0781\n",
      "49 0.06738333333333334 0.0778\n",
      "50 0.06841666666666667 0.0785\n",
      "51 0.067 0.0773\n",
      "52 0.06721666666666666 0.0773\n",
      "53 0.06761666666666667 0.0781\n",
      "54 0.06691666666666667 0.078\n",
      "55 0.06626666666666667 0.0774\n",
      "56 0.06691666666666667 0.078\n",
      "57 0.06655 0.0777\n",
      "58 0.06685 0.0786\n",
      "59 0.06728333333333333 0.078\n",
      "60 0.06618333333333333 0.077\n",
      "61 0.06663333333333334 0.0768\n",
      "62 0.0669 0.0775\n",
      "63 0.0665 0.0784\n",
      "64 0.06738333333333334 0.0794\n",
      "65 0.06603333333333333 0.077\n",
      "66 0.0666 0.0787\n",
      "67 0.06611666666666667 0.0783\n",
      "68 0.06701666666666667 0.0787\n",
      "69 0.06673333333333334 0.0772\n",
      "70 0.06646666666666666 0.078\n",
      "71 0.0657 0.0767\n",
      "72 0.0658 0.0785\n",
      "73 0.0667 0.0806\n",
      "74 0.06601666666666667 0.0778\n",
      "75 0.06568333333333333 0.0772\n",
      "76 0.06653333333333333 0.0795\n",
      "77 0.06545 0.0771\n",
      "78 0.06585 0.0791\n",
      "79 0.06518333333333333 0.0775\n",
      "80 0.06503333333333333 0.0779\n",
      "81 0.06553333333333333 0.078\n",
      "82 0.06526666666666667 0.0782\n",
      "83 0.0653 0.0771\n",
      "84 0.06575 0.0798\n",
      "85 0.06485 0.0776\n",
      "86 0.06515 0.0779\n",
      "87 0.06691666666666667 0.0807\n",
      "88 0.06561666666666667 0.077\n",
      "89 0.0661 0.0797\n",
      "90 0.0655 0.0779\n",
      "91 0.06485 0.078\n",
      "92 0.06528333333333333 0.0786\n",
      "93 0.06535 0.079\n",
      "94 0.06563333333333334 0.0786\n",
      "95 0.06496666666666667 0.0787\n",
      "96 0.06475 0.0778\n",
      "97 0.06473333333333334 0.0778\n",
      "98 0.06466666666666666 0.0776\n",
      "99 0.06498333333333334 0.0771\n",
      "100 0.06453333333333333 0.0772\n",
      "Training error / cost : 0.0645 / 15.8908\n",
      "Test error / cost : 0.0772 / 7.3881\n",
      "0.06453333333333333\n",
      "0.0767\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.6\n",
    "nepochs = 100\n",
    "w,b = initialize_params(28*28, 10)\n",
    "#params, learning_curves = optimize(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate)\n",
    "params, learning_curves = optimize_mbgd(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate, batchsize=500, smooth=False)\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_test\"], label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.0,0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_test\"], label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.00,0.15])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_w\"], label=\"dw\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_b\"], label=\"db\")\n",
    "plt.ylabel('Step Size')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "#plt.axis([0,xmax,0.00001,0.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict(params['w'], params['b'], x_test)\n",
    "yhat = np.argmax(y_pred, axis=0)\n",
    "indices = np.where(yhat!=y_test)[1]\n",
    "print(len(indices))\n",
    "\n",
    "plot_digits(x_test, y_test, indices[10:25])\n",
    "print(y_test[:,indices[10:25]])\n",
    "print(yhat[indices[10:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as 8x8 Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = params['w']\n",
    "biases = params['b']\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(20,4*rows))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(np.reshape(weights[i], (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit %i'%i, fontsize = 12)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(range(10), [biases[i] for i in range(10)], '+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = params['w']\n",
    "b = params['b']\n",
    "y_pred = predict(W,b,x_test)\n",
    "errorrates = []\n",
    "errtotal = 0.0\n",
    "count = 0\n",
    "for digit in range(10):\n",
    "    mask = np.where(y_test[0]==digit)[0]\n",
    "    y_test_digit = y_test[0,mask]\n",
    "    x_test_digit = x_test[:,mask]\n",
    "    y_pred_digit = y_pred[:,mask]\n",
    "    lendigit = x_test_digit.shape[1]\n",
    "    rate = error_rate(y_pred_digit, y_test_digit)\n",
    "    errorrates.append(rate)\n",
    "    errtotal += rate*lendigit\n",
    "    count += lendigit\n",
    "    print(digit, rate, lendigit, rate*lendigit)\n",
    "print(errtotal/x_test.shape[1], count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = range(10)\n",
    "plt.plot(targets, errorrates,\"o\")\n",
    "plt.axis([-1,10,0.0,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
