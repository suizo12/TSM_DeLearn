{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Full classification of MNIST data.\n",
    "\n",
    "The original MNIST dataset is used.\n",
    "\n",
    "In the following, we use the following notation (see also the notations sheet):\n",
    "\n",
    "m: Number of samples <br>\n",
    "n: Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "x, y = mnist['data'], np.array(mnist['target'], dtype='int')\n",
    "\n",
    "print(\"Image Data Shape\" , x.shape)\n",
    "print(\"Label Data Shape\", y.shape)\n",
    "\n",
    "image = x[17,:]\n",
    "plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "print(np.reshape(image, (28,28)))\n",
    "print(y[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use 10'000 test samples (~14%).\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=0)\n",
    "\n",
    "# reshape: \n",
    "# for x a simple transpose is sufficient \n",
    "# (m,n) -> (n,m) where m is the number of samples and n the number of input features (pixels)\n",
    "# for y reshape the simple array to become a (1,m) array\n",
    "x_train1 = x_train0.T\n",
    "x_test1 = x_test0.T\n",
    "m_train = x_train0.shape[0]\n",
    "m_test = x_test0.shape[0]\n",
    "y_train=y_train.reshape(1,m_train)\n",
    "y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "print(\"Shape training set: \", x_train1.shape, y_train.shape)\n",
    "print(\"Shape test set:     \", x_test1.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centering later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xmax = np.max(x_train1)\n",
    "xmin = np.min(x_train1)\n",
    "print(xmin, xmax)\n",
    "x_train = x_train1 / xmax\n",
    "x_test = x_test1 / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Compute the per class probabilities for all the m samples by using a softmax layer with parameters (W, b).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array with shape (ny, nx) (with ny=10 for MNIST).\n",
    "    b -- biases, a numpy array with shape (ny,1)\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    \n",
    "    Returns:\n",
    "    A -- a numpy array of shape (ny,m) or (ny,1) with the prediction probabilities for the digits.\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return A\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([2, 3]).reshape(2,1)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([0.01587624,0.86681333,0.11731043]).reshape(A.shape)\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), 1.0, decimal=8)\n",
    "\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([[0.46831053, 0.01321289, 0.21194156, 0.01321289],\n",
    " [0.46831053, 0.26538793, 0.57611688, 0.26538793],\n",
    " [0.06337894, 0.72139918, 0.21194156, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Cross Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapey(yhat,y):\n",
    "    \"\"\"\n",
    "    Checks whether the inputs come as a list in which case it reshapes it to (1,m).\n",
    "    Implementation is sloppy...\n",
    "    \"\"\"\n",
    "    if type(yhat).__module__ == np.__name__:\n",
    "        m = yhat.size\n",
    "        yhat = yhat.reshape(1,m)\n",
    "        y = y.reshape(1,m)\n",
    "    else:\n",
    "        m = 1\n",
    "    return yhat, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    Ypred -- prediction from siftmax, a numpy array of shape (ny,m) or (ny,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Cross Entropy Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([1])\n",
    "Ypred = np.array([0.04742587,0.95257413]).reshape(2,1)\n",
    "J = cost(Ypred,Y)\n",
    "Jexp = 0.04858735\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)\n",
    "\n",
    "Y = np.array([1,1,1,0])\n",
    "Ypred = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]])\n",
    "Jexp = 0.01859102\n",
    "J = cost(Ypred,Y)\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y,n):\n",
    "    \"\"\"\n",
    "    Constructs a one-hot-vector from a given array of labels (shape (1,m)) and the number of classes n.\n",
    "    The resulting array has shape (n,m) and in row j and column i a '1' if the i-th sample had a label 'j'.\n",
    "    We assume that the labels can have values (0,1,2,...,n-1). \n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "    result = np.zeros((n,m),dtype=float)\n",
    "    result[y[0,:],np.arange(m)] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m).\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. W (key \"dW\" with shape (ny,nx)) and w.r.t. b (key \"db\" with shape (ny,1))\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###    \n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "    return gradJ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Calculation of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "Y = np.array([1,1,1,1]).reshape(1,4)\n",
    "gradJ = gradient(W,b,X,Y)\n",
    "dW = gradJ['dW']\n",
    "db = gradJ['db']\n",
    "dWexp = np.array([[ 0.28053421,0.17666947],\n",
    "                  [-0.00450948,-0.60619918],\n",
    "                  [-0.27602473,0.42952972]]).reshape(3,2)\n",
    "dbexp = np.array([0.17666947,-0.60619918,0.42952972]).reshape(3,1)\n",
    "np.testing.assert_array_almost_equal(dW,dWexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(db,dbexp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Output Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    Ypredargmax = np.argmax(Ypred, axis=0)\n",
    "    return np.sum(Y != Ypredargmax) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PIXELS = (28,28)\n",
    "COLS = 5\n",
    "def plot_digits(X,Y,indices):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with up to 8 columns\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (1, 64)\n",
    "    Y -- label (a scalar)\n",
    "    indices -- list of indices    \n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        print(\"No images to show!\")\n",
    "        return\n",
    "    cols = min(COLS, len(indices))\n",
    "    rows = len(indices)/COLS+1\n",
    "    plt.figure(figsize=(20,5*rows))\n",
    "    for index, (image, label) in enumerate(zip(X.T[indices,:], Y.T[indices,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, PIXELS), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (indices[index],label), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(nx,ny, random=False):\n",
    "    \"\"\"\n",
    "    This function creates initial values:\n",
    "    * for w a vector of zeros of shape (1,n) [random=False] or a vector of normally distributed random values [random=True] \n",
    "    * for b set to 0.\n",
    "    \n",
    "    Argument:\n",
    "    nx,ny -- number of input features and number of outputs\n",
    "\n",
    "    Returns:\n",
    "    W -- initialized weights matrix of shape (ny,nx)\n",
    "    b -- initialized bias array of shape (ny,1)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        W = np.random.randn(*(ny,nx))\n",
    "    else:\n",
    "        W = np.zeros((ny,nx))\n",
    "    b = np.zeros((ny,1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(W, b, x_train, y_train, x_test, y_test, nepochs, alpha):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    Returns the trained parameters values as dictionary (keys \"w\" and \"b\") and various quantities \n",
    "    collected during learning also as dictionary: cost on train and test set (\"cost_train\", \"cost_test\"), \n",
    "    error rate on train and test set (\"error_train\", \"error_test\"), learning speed as length of dw and db\n",
    "    multiplied by alpha with key \"step_w\" and \"step_b\", respectively.\n",
    "    The output is provided in form of dictionaries (basically, to avoid handling too many variable names in \n",
    "    functional calls).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\"     \n",
    "    # The following lists are used for tracking the learning progress so that learning curves can be plotted.\n",
    "    # Append an according value in each epoch\n",
    "    epochs = []  # fill here the epoch id (the iteration index when looping over nepochs)\n",
    "    train_costs = [] # fill here the cost on the training set  \n",
    "    test_costs = [] # fill here the cost on the test set \n",
    "    train_errors = [] # fill here the error rate on the training set\n",
    "    test_errors = [] # fill here the error rate on the test set \n",
    "    stepsize_w = [] # fill here the lenght of the gradient of the weights vector multiplied with alpha (for the training set)\n",
    "    stepsize_b = [] # fill here the absolute value of derivative wr.t. the bias multiplied with alpha (for the training set)\n",
    "        \n",
    "        \n",
    "    for i in range(nepochs):\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)    \n",
    "    \n",
    "    params = {\"w\": W, \"b\": b}    \n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "        \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_mbgd(W, b, x_train, y_train, x_test, y_test, nepochs, batchsize, alpha, smooth=False):\n",
    "    \"\"\"\n",
    "    Same as the optimize function above - except that it should implement batch gradient descent with given batchsize.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    batchsize -- size of mini-batch\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    smooth -- if true the measures for the learning curves are smoothed per epoch\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\" \n",
    "    epochs = []\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    stepsize_w = []\n",
    "    stepsize_b = []\n",
    "        \n",
    "    n = x_train.shape[0] # number of inputs\n",
    "    m = x_train.shape[1] # number of samples\n",
    "    mb = int(m/batchsize)\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)\n",
    "        \n",
    "    params = {\"w\": W, \"b\": b}\n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "    \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.6\n",
    "nepochs = 100\n",
    "w,b = initialize_params(28*28, 10)\n",
    "#params, learning_curves = optimize(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate)\n",
    "params, learning_curves = optimize_mbgd(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate, batchsize=500, smooth=False)\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_test\"], label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.0,0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_test\"], label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.00,0.15])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_w\"], label=\"dw\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_b\"], label=\"db\")\n",
    "plt.ylabel('Step Size')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "#plt.axis([0,xmax,0.00001,0.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict(params['w'], params['b'], x_test)\n",
    "yhat = np.argmax(y_pred, axis=0)\n",
    "indices = np.where(yhat!=y_test)[1]\n",
    "print(len(indices))\n",
    "\n",
    "plot_digits(x_test, y_test, indices[10:25])\n",
    "print(y_test[:,indices[10:25]])\n",
    "print(yhat[indices[10:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as 8x8 Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = params['w']\n",
    "biases = params['b']\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(20,4*rows))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(np.reshape(weights[i], (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit %i'%i, fontsize = 12)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(range(10), [biases[i] for i in range(10)], '+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = params['w']\n",
    "b = params['b']\n",
    "y_pred = predict(W,b,x_test)\n",
    "errorrates = []\n",
    "errtotal = 0.0\n",
    "count = 0\n",
    "for digit in range(10):\n",
    "    mask = np.where(y_test[0]==digit)[0]\n",
    "    y_test_digit = y_test[0,mask]\n",
    "    x_test_digit = x_test[:,mask]\n",
    "    y_pred_digit = y_pred[:,mask]\n",
    "    lendigit = x_test_digit.shape[1]\n",
    "    rate = error_rate(y_pred_digit, y_test_digit)\n",
    "    errorrates.append(rate)\n",
    "    errtotal += rate*lendigit\n",
    "    count += lendigit\n",
    "    print(digit, rate, lendigit, rate*lendigit)\n",
    "print(errtotal/x_test.shape[1], count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = range(10)\n",
    "plt.plot(targets, errorrates,\"o\")\n",
    "plt.axis([-1,10,0.0,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
