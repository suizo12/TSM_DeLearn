{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Full classification of MNIST data.\n",
    "\n",
    "The original MNIST dataset is used.\n",
    "\n",
    "In the following, we use the following notation (see also the notations sheet):\n",
    "\n",
    "m: Number of samples <br>\n",
    "n: Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (70000, 784)\n",
      "Label Data Shape (70000,)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  22 183 252 254 252 252\n",
      "  252  76   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  85  85 168 250 250 252 250 250\n",
      "  250 250  71   0  43  85  14   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 107 252 250 250 250 250 252 250 250\n",
      "  250 250 210   0 127 250 146   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 114 237 252 250 250 250 250 252 250 250\n",
      "  250 250 210   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0   0   0 107 237 250 252 250 250 250  74  41  41  41\n",
      "   41 217  34   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0   0  15 148 252 252 254 238 105   0   0   0   0   0\n",
      "    0   0   0   0 128 252 252   0   0   0]\n",
      " [  0   0   0   0   0  15 140 250 250 250 167 111   0   0   0   0   0   0\n",
      "    0   0   0   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0  43 250 250 250 250   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 127 250 250   0   0   0]\n",
      " [  0   0   0   0   0 183 250 250 250 110   0   0   0   0   0   0   0   0\n",
      "    0   0   0  57 210 250 250   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 110   7   0   0   0   0   0   0   0   0\n",
      "    0   0   0  85 250 250 250   0   0   0]\n",
      " [  0   0   0   0   0 254 252 252  83   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  86 252 252 217   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 138  14   0   0   0   0   0   0   0   0\n",
      "    0   0  15 140 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 250  41   0   0   0   0   0   0   0   0\n",
      "    0   0  43 250 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0 252 250 250 250 181   0   0   0   0   0   0   0   0\n",
      "    0   0 183 250 250 250  41   0   0   0]\n",
      " [  0   0   0   0   0  76 250 250 250 250   0   0   0   0   0   0   0   0\n",
      "    0 177 252 250 250 110   7   0   0   0]\n",
      " [  0   0   0   0   0  36 224 252 252 252 219  43  43  43   7   0  15  43\n",
      "  183 252 255 252 126   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 250 250 250 252 250 250 250 111  86 140 250\n",
      "  250 250 252 222  83   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  42 188 250 250 252 250 250 250 250 252 250 250\n",
      "  250 250 126  83   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 127 250 250 252 250 250 250 250 252 250 250\n",
      "  137  83   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  21  41 217 252 250 250 250 250 217  41  41\n",
      "   14   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "x, y = mnist['data'], np.array(mnist['target'], dtype='int')\n",
    "\n",
    "print(\"Image Data Shape\" , x.shape)\n",
    "print(\"Label Data Shape\", y.shape)\n",
    "\n",
    "image = x[17,:]\n",
    "plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "print(np.reshape(image, (28,28)))\n",
    "print(y[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use 10'000 test samples (~14%).\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training set:  (784, 60000) (1, 60000)\n",
      "Shape test set:      (784, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=0)\n",
    "\n",
    "# reshape: \n",
    "# for x a simple transpose is sufficient \n",
    "# (m,n) -> (n,m) where m is the number of samples and n the number of input features (pixels)\n",
    "# for y reshape the simple array to become a (1,m) array\n",
    "x_train1 = x_train0.T\n",
    "x_test1 = x_test0.T\n",
    "m_train = x_train0.shape[0]\n",
    "m_test = x_test0.shape[0]\n",
    "y_train=y_train.reshape(1,m_train)\n",
    "y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "print(\"Shape training set: \", x_train1.shape, y_train.shape)\n",
    "print(\"Shape test set:     \", x_test1.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centering later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xmax = np.max(x_train1)\n",
    "xmin = np.min(x_train1)\n",
    "print(xmin, xmax)\n",
    "x_train = x_train1 / xmax\n",
    "x_test = x_test1 / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Compute the per class probabilities for all the m samples by using a softmax layer with parameters (W, b).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array with shape (ny, nx) (with ny=10 for MNIST).\n",
    "    b -- biases, a numpy array with shape (ny,1)\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    \n",
    "    Returns:\n",
    "    A -- a numpy array of shape (ny,m) or (ny,1) with the prediction probabilities for the digits.\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    A = softmax(np.dot(X.T, W.T)+b.T)\n",
    "    ### END YOUR CODE ###\n",
    "    return A.T\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -1\n",
      "s [[0.73105858 0.26894142]]\n",
      "0 1\n",
      "s [[0.26894142 0.73105858]]\n",
      "-1 1\n",
      "s [[0.26894142 0.73105858]]\n",
      "(3, 2)\n",
      "(2, 1)\n",
      "A [[0.01587624 0.86681333 0.11731043]]\n",
      "sum 0.9999999999999999\n",
      "1 -1\n",
      "s [[0.26894142 0.73105858]\n",
      " [0.88079708 0.11920292]\n",
      " [0.5        0.5       ]\n",
      " [0.88079708 0.11920292]]\n",
      "0 1\n",
      "s [[0.73105858 0.26894142]\n",
      " [0.11920292 0.88079708]\n",
      " [0.5        0.5       ]\n",
      " [0.11920292 0.88079708]]\n",
      "-1 1\n",
      "s [[0.73105858 0.26894142]\n",
      " [0.11920292 0.88079708]\n",
      " [0.5        0.5       ]\n",
      " [0.11920292 0.88079708]]\n",
      "(3, 2)\n",
      "(2, 4)\n",
      "A [[0.46831053 0.46831053 0.06337894]\n",
      " [0.01321289 0.26538793 0.72139918]\n",
      " [0.21194156 0.57611688 0.21194156]\n",
      " [0.01321289 0.26538793 0.72139918]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([2, 3]).reshape(2,1)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([0.01587624,0.86681333,0.11731043]).reshape(A.shape)\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "print('sum', np.sum(A))\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), 1.0, decimal=8)\n",
    "\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([[0.46831053, 0.01321289, 0.21194156, 0.01321289],\n",
    " [0.46831053, 0.26538793, 0.57611688, 0.26538793],\n",
    " [0.06337894, 0.72139918, 0.21194156, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Cross Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapey(yhat,y):\n",
    "    \"\"\"\n",
    "    Checks whether the inputs come as a list in which case it reshapes it to (1,m).\n",
    "    Implementation is sloppy...\n",
    "    \"\"\"\n",
    "    if type(yhat).__module__ == np.__name__:\n",
    "        m = yhat.size\n",
    "        yhat = yhat.reshape(1,m)\n",
    "        y = y.reshape(1,m)\n",
    "    else:\n",
    "        m = 1\n",
    "    return yhat, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    Ypred -- prediction from softmax, a numpy array of shape (ny,m) or (ny,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    m = len(Y)\n",
    "    \n",
    "    # see cost function on slide 13\n",
    "    #Select the right index.\n",
    "    # The idea is, Y represents the right label, for the secon example the labels are 1, 1, 1, 0\n",
    "    # ypred is (rounded 2 decimals) [[0.02 0.01 0.05 1.  ]\n",
    "    #                                [0.98 0.99 0.95 0.  ]]\n",
    "    # which means that for the first h the % that it is a zero is 2% and that it is a 1 is 98%\n",
    "    # now you select the value from your Y value meaning 0.98, 0.99, 0.95, 1  with Ypred.T[np.arange(m), Y]\n",
    "    ### END YOUR CODE ###\n",
    "    return - (np.log(Ypred.T[np.arange(m), Y]).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Cross Entropy Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second test\n",
      "[[0.02 0.01 0.05 1.  ]\n",
      " [0.98 0.99 0.95 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1])\n",
    "Ypred = np.array([0.04742587,0.95257413]).reshape(2,1)\n",
    "J = cost(Ypred,Y)\n",
    "Jexp = 0.04858735\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)\n",
    "print('second test')\n",
    "Y = np.array([1,1,1,0])\n",
    "Ypred = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]])\n",
    "print(np.round(Ypred, 2))\n",
    "Jexp = 0.01859102\n",
    "J = cost(Ypred,Y)\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y,n):\n",
    "    \"\"\"\n",
    "    Constructs a one-hot-vector from a given array of labels (shape (1,m)) and the number of classes n.\n",
    "    The resulting array has shape (n,m) and in row j and column i a '1' if the i-th sample had a label 'j'.\n",
    "    We assume that the labels can have values (0,1,2,...,n-1). \n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "    result = np.zeros((n,m),dtype=float)\n",
    "    result[y[0,:],np.arange(m)] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m).\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. W (key \"dW\" with shape (ny,nx)) and w.r.t. b (key \"db\" with shape (ny,1))\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###    \n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "    return gradJ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Calculation of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "Y = np.array([1,1,1,1]).reshape(1,4)\n",
    "gradJ = gradient(W,b,X,Y)\n",
    "dW = gradJ['dW']\n",
    "db = gradJ['db']\n",
    "dWexp = np.array([[ 0.28053421,0.17666947],\n",
    "                  [-0.00450948,-0.60619918],\n",
    "                  [-0.27602473,0.42952972]]).reshape(3,2)\n",
    "dbexp = np.array([0.17666947,-0.60619918,0.42952972]).reshape(3,1)\n",
    "np.testing.assert_array_almost_equal(dW,dWexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(db,dbexp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Output Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    Ypredargmax = np.argmax(Ypred, axis=0)\n",
    "    return np.sum(Y != Ypredargmax) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PIXELS = (28,28)\n",
    "COLS = 5\n",
    "def plot_digits(X,Y,indices):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with up to 8 columns\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (1, 64)\n",
    "    Y -- label (a scalar)\n",
    "    indices -- list of indices    \n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        print(\"No images to show!\")\n",
    "        return\n",
    "    cols = min(COLS, len(indices))\n",
    "    rows = len(indices)/COLS+1\n",
    "    plt.figure(figsize=(20,5*rows))\n",
    "    for index, (image, label) in enumerate(zip(X.T[indices,:], Y.T[indices,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, PIXELS), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (indices[index],label), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(nx,ny, random=False):\n",
    "    \"\"\"\n",
    "    This function creates initial values:\n",
    "    * for w a vector of zeros of shape (1,n) [random=False] or a vector of normally distributed random values [random=True] \n",
    "    * for b set to 0.\n",
    "    \n",
    "    Argument:\n",
    "    nx,ny -- number of input features and number of outputs\n",
    "\n",
    "    Returns:\n",
    "    W -- initialized weights matrix of shape (ny,nx)\n",
    "    b -- initialized bias array of shape (ny,1)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        W = np.random.randn(*(ny,nx))\n",
    "    else:\n",
    "        W = np.zeros((ny,nx))\n",
    "    b = np.zeros((ny,1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(W, b, x_train, y_train, x_test, y_test, nepochs, alpha):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    Returns the trained parameters values as dictionary (keys \"w\" and \"b\") and various quantities \n",
    "    collected during learning also as dictionary: cost on train and test set (\"cost_train\", \"cost_test\"), \n",
    "    error rate on train and test set (\"error_train\", \"error_test\"), learning speed as length of dw and db\n",
    "    multiplied by alpha with key \"step_w\" and \"step_b\", respectively.\n",
    "    The output is provided in form of dictionaries (basically, to avoid handling too many variable names in \n",
    "    functional calls).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\"     \n",
    "    # The following lists are used for tracking the learning progress so that learning curves can be plotted.\n",
    "    # Append an according value in each epoch\n",
    "    epochs = []  # fill here the epoch id (the iteration index when looping over nepochs)\n",
    "    train_costs = [] # fill here the cost on the training set  \n",
    "    test_costs = [] # fill here the cost on the test set \n",
    "    train_errors = [] # fill here the error rate on the training set\n",
    "    test_errors = [] # fill here the error rate on the test set \n",
    "    stepsize_w = [] # fill here the lenght of the gradient of the weights vector multiplied with alpha (for the training set)\n",
    "    stepsize_b = [] # fill here the absolute value of derivative wr.t. the bias multiplied with alpha (for the training set)\n",
    "        \n",
    "        \n",
    "    for i in range(nepochs):\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)    \n",
    "    \n",
    "    params = {\"w\": W, \"b\": b}    \n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "        \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_mbgd(W, b, x_train, y_train, x_test, y_test, nepochs, batchsize, alpha, smooth=False):\n",
    "    \"\"\"\n",
    "    Same as the optimize function above - except that it should implement batch gradient descent with given batchsize.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    X -- input data of size (nx,m) or (nx,1)\n",
    "    Y -- output labels - a numpy array with shape (1,m) or a scalar.\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    batchsize -- size of mini-batch\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    smooth -- if true the measures for the learning curves are smoothed per epoch\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\" \n",
    "    epochs = []\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    stepsize_w = []\n",
    "    stepsize_b = []\n",
    "        \n",
    "    n = x_train.shape[0] # number of inputs\n",
    "    m = x_train.shape[1] # number of samples\n",
    "    mb = int(m/batchsize)\n",
    "    \n",
    "    for i in range(nepochs):\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        print(i+1, train_error, test_error)\n",
    "        \n",
    "    params = {\"w\": W, \"b\": b}\n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "    \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.6\n",
    "nepochs = 100\n",
    "w,b = initialize_params(28*28, 10)\n",
    "#params, learning_curves = optimize(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate)\n",
    "params, learning_curves = optimize_mbgd(w, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate, batchsize=500, smooth=False)\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"cost_test\"], label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.0,0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_train\"], label=\"train\")\n",
    "plt.plot(learning_curves[\"epochs\"], learning_curves[\"error_test\"], label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.00,0.15])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.min(learning_curves[\"error_train\"]))\n",
    "print(np.min(learning_curves[\"error_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_w\"], label=\"dw\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_b\"], label=\"db\")\n",
    "plt.ylabel('Step Size')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "#plt.axis([0,xmax,0.00001,0.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict(params['w'], params['b'], x_test)\n",
    "yhat = np.argmax(y_pred, axis=0)\n",
    "indices = np.where(yhat!=y_test)[1]\n",
    "print(len(indices))\n",
    "\n",
    "plot_digits(x_test, y_test, indices[10:25])\n",
    "print(y_test[:,indices[10:25]])\n",
    "print(yhat[indices[10:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as 8x8 Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = params['w']\n",
    "biases = params['b']\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(20,4*rows))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(np.reshape(weights[i], (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit %i'%i, fontsize = 12)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(range(10), [biases[i] for i in range(10)], '+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = params['w']\n",
    "b = params['b']\n",
    "y_pred = predict(W,b,x_test)\n",
    "errorrates = []\n",
    "errtotal = 0.0\n",
    "count = 0\n",
    "for digit in range(10):\n",
    "    mask = np.where(y_test[0]==digit)[0]\n",
    "    y_test_digit = y_test[0,mask]\n",
    "    x_test_digit = x_test[:,mask]\n",
    "    y_pred_digit = y_pred[:,mask]\n",
    "    lendigit = x_test_digit.shape[1]\n",
    "    rate = error_rate(y_pred_digit, y_test_digit)\n",
    "    errorrates.append(rate)\n",
    "    errtotal += rate*lendigit\n",
    "    count += lendigit\n",
    "    print(digit, rate, lendigit, rate*lendigit)\n",
    "print(errtotal/x_test.shape[1], count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = range(10)\n",
    "plt.plot(targets, errorrates,\"o\")\n",
    "plt.axis([-1,10,0.0,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
