{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO NOTEBOOK - RETRIEVAL CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION\n",
    "\n",
    "In this exercise we will see step by step the process of building a retrieval chatbot, preparing the dataset we want to work with, create a model and train the model to get a dialog system with the ability to answer to the users questions. \n",
    "\n",
    "As you know from the slides, a retrieval chatbot doesn't generate an answer from scratch. It receives a question (the user input), use some heuristic to retrieve a set of candidates to be answer to that question and finally it selects the best one as final answer. Our goal in this exercise is to have a chatbot able to perform this task in a closed domain: Ubuntu customer support.\n",
    "\n",
    "## 1.1. Dataset\n",
    "In this case we are going to work with the Ubuntu Corpus (https://arxiv.org/pdf/1506.08909) to create a retrieval chatbot capable of answering technical support questions about the well known OS Ubuntu. The set can be downloaded in **https://drive.google.com/file/d/0B_bZck-ksdkpVEtVc1R6Y01HMWM/view**. It consists of dialogs extracted from the forums, so each conversation has two participants.\n",
    "\n",
    "In the training dataset, the dialogs have been processed to obtain a series of pairs **context** - **utterance**. Each sentence of the dialog is going to appear as an utterance in one of the pairs, while the context of that especific pair is formed by the sentences previous to the utterance.\n",
    "\n",
    "The testing dataset is different, as we have each sentence of the dialog as **context** and then the following sentence of the dialog (from the other user) as **utterance**. In addition, each pair has also 9 **distractors**, false utterances selected randomly from the dataset. Given a context, the model will receive the correct utterance and the distractors as candidates to be answers, and the model should be able to give the correct one a better score than the others.\n",
    "\n",
    "\n",
    "## 1.2. Model\n",
    "The architecture of the neural network is called the Dual Encoder LSTM. It's described also in the paper mentioned before, and it's formed by two encoders. One of them encodes the question we want to answer and the other one the candidate to be the answer. The output of the architecture is a score between 0 and 1. The closer the score is to 1, the better the answer is for that question.\n",
    "\n",
    "<img src=\"dualencoder.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Requirements\n",
    "\n",
    "First of all, we need to install the libraries required to complete this project. The most important are:\n",
    "\n",
    "* Python 3.6.5\n",
    "* PyTorch 1.x\n",
    "* nltk\n",
    "\n",
    "Once installed, import them into the project and we are ready to start.\n",
    "\n",
    "Make sure you set the value of your data_dir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "data_dir = \"/Users/taamucl2/Swisscom/_Research-projects/_chatbot_course/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is provided as CSV files we imported the required library to read that data.\n",
    "- We have created a reader for the CSV file.\n",
    "- Use the reader to create a list of rows.\n",
    "- Keep in mind the first row of a CSV contains the column headers.\n",
    "- Finally use the random library to shuffle the training list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = csv.reader(open(data_dir + 'train.csv'))\n",
    "all_rows = #Todo\n",
    "#then clear the headers:\n",
    "rows = #Todo\n",
    "#Now randomize the rows\n",
    "random. .. #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = # TODO\n",
    "valid = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the vocabulary and the pre-trained word embeddings, in our case glove.\n",
    "We need to create tow dictionaries in python. Additionally, we will load a stemmer that we will need for the preprocessing functions\n",
    "- For the vocab, we are going to iterate over all words in the list, and set them as keys, while the value is going to be the ordinal number of the word\n",
    "- Our dictionary uses the ordinal number of the wors as a key while the value is the pre-trained word embedding. This function is already given to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "Given a dictionary like:\n",
    "- **hello**\n",
    "- **hi**\n",
    "- **bot**\n",
    "\n",
    "The output of load_vocab will be:\n",
    "- **{'hello': 0, 'hi': 1, 'bot': 2}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    return # TODO\n",
    "\n",
    "\n",
    "def load_glove_embeddings(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    embeddings = {}\n",
    "    for line in lines:\n",
    "        word = line.split()[0] \n",
    "        embedding = list(map(float, line.split()[1:]))\n",
    "        if word in vocab:\n",
    "            embeddings[vocab[word]] = embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "vocab = load_vocab(data_dir + 'vocabulary.txt')\n",
    "glove_embeddings = load_glove_embeddings(data_dir + 'glove.6B.100d.txt')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to define our preprocessing functions. \n",
    "\n",
    "- The numberize function takes as input a string. Splits the string and uses our vocabulary dictionary to create a vector of the string such that work is represented by a number. If the string is short we also pad the vector. \n",
    "\n",
    "- In the process_train function, you need to return a tuple of numberized context vector, numberized response vector and integer variable for the label\n",
    "\n",
    "- In the process_valid function, you need to return a tuple of numberized context vector, numberized response vector, and list of numberized distractor vectors\n",
    "\n",
    "- The process_predict_embed function does 3 things to a response. First, it tokenizes it, then we stem each token and finally we generated a numeric vector of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "Given the row tuple:\n",
    "- **(\"hello bot\", \"hi\", \"1\")**\n",
    "\n",
    "The output of process_train will be:\n",
    "\n",
    "- **([0,..,0,222,909],  [0,..,0,137], 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numberize(inp):\n",
    "    inp = inp.split()\n",
    "    result = list(map(lambda k: vocab.get(k, 0), inp))[-160:]\n",
    "    if len(result) < 160:\n",
    "        result = [0] * (160 - len(result)) + result\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_train(row):\n",
    "    context, response, label = row\n",
    "\n",
    "    context = # TODO\n",
    "    response = # TODO\n",
    "    label = # TODO\n",
    "\n",
    "    return context, response, label\n",
    "\n",
    "def process_valid(row):\n",
    "    context = # TODO\n",
    "    response = # TODO\n",
    "    distractors = # TODO\n",
    "\n",
    "    context = # TODO\n",
    "    response = # TODO\n",
    "    distractors = # TODO\n",
    "\n",
    "    return context, response, distractors\n",
    "\n",
    "def process_predict_embed(response):\n",
    "    response = ' '.join(list(map(stemmer.stem, nltk.word_tokenize(response))))\n",
    "    response = numberize(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Description of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data initialized and all preprocessing functions ready, it's time to start defining the <b>Graph</b> of the model.\n",
    "\n",
    "We are going to define our model with two classes. First, we will define the Encoder. For this, we are going to extend the <b>nn.Module class of torch</b>\n",
    "\n",
    "There are 3 functions within our Class:\n",
    "\n",
    "- _init_ is the default python method to create an instance, in this method we pass the basic parameters such as the number of network layers, the layers hidden size etc. In this model, we are going to use **LSTM**, although GRU could also work. \n",
    "\n",
    "- The forward is the most important method in the class and defines the computation that this module performs when it is given an input. In our case since it is an encoder, we take as input a numerzied vector of a string. Then using the embedding dictionary for each token we get its word embedding. Finally, we pass all the tokes to the RNN and return the outputs and hidden states.\n",
    "\n",
    "- Finally, we define an init_weights to that is called by _init_ and gives us more control over how the parameters of the RNN are initialized. Additionally, we initialize the self.embedding.weight with the dictionary of word embeddings we already loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            vocab_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=True,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size // self.num_directions\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size, sparse=False, padding_idx=0)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, inps):\n",
    "        embs = self.embedding(inps)\n",
    "        outputs, hiddens = self.rnn(embs)\n",
    "        return outputs, hiddens\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.orthogonal_(self.rnn.weight_ih_l0)\n",
    "        init.uniform_(self.rnn.weight_hh_l0, a=-0.01, b=0.01)\n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.input_size)\n",
    "        init.uniform_(embedding_weights, a=-0.25, b=0.25)\n",
    "        for k, v in glove_embeddings.items():\n",
    "            embedding_weights[k] = torch.FloatTensor(v)\n",
    "        embedding_weights[0] = torch.FloatTensor([0] * self.input_size)\n",
    "        del self.embedding.weight\n",
    "        self.embedding.weight = nn.Parameter(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined an Encoder module, we can define our full DualEncoder Module.\n",
    "\n",
    "Again we extend the nn.Module class and we implement the required methods:\n",
    "\n",
    "- In the __init__ Module we pass an instance of an Encoder Module. Then based on the size of the Encoder output we define our trainable square matrix. We also define the final dense layer\n",
    "\n",
    "- For the forward method first, we generate the encoding for both the contexts and responses. Finally, the **prediction** for a given context will be obtained by multiplying the context encoded by the *prediction matrix* M, that will be trained. However, it isn't this prediction what we want to get. Now, we can multiply it to the real encoded utterance and apply the *sigmoid* function to get the probability of the pair context-utterance being correct. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dualencoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        h_size = self.encoder.hidden_size * self.encoder.num_directions\n",
    "        M = torch.FloatTensor(h_size, h_size)\n",
    "        init.normal_(M)\n",
    "        self.M = nn.Parameter(\n",
    "            M,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        \n",
    "        dense_dim = 2 * self.encoder.hidden_size\n",
    "        self.dense = nn.Linear(dense_dim, dense_dim)\n",
    "      \n",
    "\n",
    "    def forward(self, contexts, responses):\n",
    "    \n",
    "        context_os, context_hs = self.encoder(contexts)\n",
    "        response_os, response_hs = self.encoder(responses)\n",
    "        \n",
    "        context_hs = context_hs[0]\n",
    "        response_hs = response_hs[0]\n",
    "\n",
    "        results = []\n",
    "        response_encodings = []\n",
    "\n",
    "        h_size = self.encoder.hidden_size * self.encoder.num_directions\n",
    "        for i in range(len(context_hs[0])):\n",
    "            context_h = context_os[i][-1].view(1, h_size)\n",
    "            response_h = response_os[i][-1].view(h_size, 1)\n",
    "\n",
    "            ans = torch.mm(torch.mm(context_h, self.M), response_h)[0][0]\n",
    "            results.append(torch.sigmoid(ans))\n",
    "            response_encodings.append(response_h)\n",
    "\n",
    "        results = torch.stack(results)\n",
    "\n",
    "        return results, response_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create an instance of the model. You can play around at home, with the size of the model. For now, we keep them fixed so you can load our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Encoder(\n",
    "    input_size=100,  # embedding dim\n",
    "    hidden_size=300,  # rnn dim\n",
    "    vocab_size=91620,  # vocab size\n",
    "    bidirectional=False,  \n",
    ")\n",
    "\n",
    "model = DualEncoder(encoder_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some function that will help us with training and testing. Our dataset is too large to fit all at once in our model. Your PC will just run out of RAM while performing the computation. For that reason, we will feed the data to our model in small parts (batches).\n",
    "\n",
    "- We need a function to return batches of rows of constant size. It takes 2 parameters the batch_number and the batch_size. \n",
    "\n",
    "- Be careful to not set the start index with a size greater than the length of the list. if the current batch index is too large, you should iterate again. Remember that rows is a global variable.\n",
    "\n",
    "- The get validation function returns the full validation set or a set of the given size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_num, batch_size):\n",
    "    start = # TODO\n",
    "    return rows[start:start+batch_size]\n",
    "\n",
    "def get_validation(num=None):\n",
    "    if num is None:\n",
    "        return valid\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "Given the input:\n",
    "- **(10, 1)**\n",
    "\n",
    "The output of **get_batch** will be:\n",
    "- [[\"be i suppos to get a question about the mode i want to be avail dure xserver-xfree86 ubuntu7 's ... __eou__ __eot__\",\n",
    "  'ok __eou__',\n",
    "  '1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training we must minimize the **mean loss** for each batch. The chosen loss function is the **cross entropy**, as in the training dataset we have labelled whether each utterance belongs to the context. \n",
    "\n",
    "Thanks to that, if the label is 1 (the pair is correct) the loss will be very close to 0 only if the score given is high, penalizing the mistake. The same works for the other case, being the label 0 (the pair is wrong), if the score is high then it will be penalized as the loss will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_steps = 10\n",
    "batch_size = 512\n",
    "evaluate_batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the optim pacakge of torch to define an optimizer. We suggest you use Adam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can proceed to training we need to define an evaltion function. We need this because we want to monitor the perfomance of our model during training\n",
    "\n",
    "Remember the estructure:\n",
    "* Context\n",
    "* Correct utterance\n",
    "* Nine distractors (wrong utterances)\n",
    "\n",
    "The evaluation is based on the function **recall@k**, being k the size of the subset selected. In other words, for each context the model will evaluate all 10 possible utterances and assign a score to each of them. For recall@1 only is correct if the best score is the correct utterance, for recall@5 it's considered correct if the correct utterance is between the 5 best scores, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, size=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a subset of vallidation set.\n",
    "    \"\"\"\n",
    "    valid = list(map(process_valid, get_validation(size)))\n",
    "\n",
    "    count = [0] * 10\n",
    "\n",
    "    for e in valid:\n",
    "        context, response, distractors = e\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            cs = Variable(torch.stack([torch.LongTensor(context) for i in range(10)], 0))\n",
    "        rs = [torch.LongTensor(response)]\n",
    "        rs += [torch.LongTensor(distractor) for distractor in distractors]\n",
    "        with torch.no_grad():\n",
    "            rs = Variable(torch.stack(rs, 0))\n",
    "\n",
    "        results, responses = model(cs, rs)\n",
    "        results = [e.data.cpu().numpy() for e in results]\n",
    "\n",
    "        better_count = sum(1 for val in results[1:] if val >= results[0])\n",
    "        count[better_count] += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to iterate in a loop for number of steps we definied. We left a copule of things missing the for loop. \n",
    "Look at the comments and fill in the missing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    \n",
    "    # First we need to get the batch for the new step\n",
    "    batch = get_batch(i, batch_size) \n",
    "    # Use the process_train function to generate and make a list with all the elements in the batch\n",
    "    batch_list = list(map(process_train, batch))\n",
    "    count = 0\n",
    "\n",
    "    cs = []\n",
    "    rs = []\n",
    "    ys = []\n",
    "\n",
    "    for c, r, y in batch_list:\n",
    "        count += 1\n",
    "\n",
    "        # Forward pass: compute predicted y by passing x to model\n",
    "        # Convert all c,r,y to tensors and append them to the defined lists\n",
    "        \n",
    "        # ATTENTION: think about the types\n",
    "        # ATTENTION: pass y as an array\n",
    "        cs.append(# TODO)\n",
    "        rs.append(# TODO)\n",
    "        ys.append(# TODO)\n",
    "\n",
    "\n",
    "    cs = Variable(torch.stack(cs, 0))\n",
    "    rs = Variable(torch.stack(rs, 0))\n",
    "    ys = Variable(torch.stack(ys, 0))\n",
    "\n",
    "    y_preds, responses = model(cs, rs)\n",
    "\n",
    "    # Compute loss\n",
    "    # Think about which parameters you need to use to comput the loss.\n",
    "    # You minght need to use .view one of the inputs to avoid missmatch in dimensions\n",
    "    loss = loss_fn(# TODO)\n",
    "    print(i, loss.data.item())\n",
    "    \n",
    "\n",
    "    \n",
    "    # Every 100 Steps we evaluate the model with a batch_size evaluation set\n",
    "    if i % 100 == 0:\n",
    "        res = evaluate(model, size=evaluate_batch_size)\n",
    "        print(i)\n",
    "        print(\"1 in 10: %0.2f, 2 in 10: %0.2f, 5 in 10: %0.2f\" % (\n",
    "            res[0] / evaluate_batch_size,\n",
    "            sum(res[:2]) / evaluate_batch_size,\n",
    "            sum(res[:5]) / evaluate_batch_size,\n",
    "        ))\n",
    "        \n",
    "\n",
    "    # Every 1000 Steps we evaluate the model with a 2000 sample evaluation set    \n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        res = evaluate(model, size=2000)\n",
    "\n",
    "        one_in = res[0] / 2000\n",
    "        two_in = sum(res[:2]) / 2000\n",
    "        three_in = sum(res[:5]) / 2000\n",
    "\n",
    "        print(\"!!!!!!!!!!\")\n",
    "        print(\"1 in 10: %0.2f, 2 in 10: %0.2f, 5 in 10: %0.2f\" % (\n",
    "            one_in,\n",
    "            two_in,\n",
    "            three_in,\n",
    "        ))\n",
    "       \n",
    "    \n",
    "    \n",
    "    # Finaly update the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del loss, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything is ready for the training!** Note that on the original experiment they trained for 20000 steps, which can take long (about 30-40 hours) without a GPU. Feel free to change the number of steps, although the results can be notably worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to remember that the main goal of this course is to be able to build a chatbot that it's able to interact with human beings. That means that it should be able to **give answers to questions outside the dataset**. For that, everytime a question is asked we can retrieve a set of possible answers and pass them by the model to obtain the score. After all the process is gone, we select the one with best score as the answer that will be returned to the user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_val(context, response):\n",
    "    c_num = process_predict_embed(context)\n",
    "    r_num = process_predict_embed(response)\n",
    "    c = Variable(torch.LongTensor([c_num]), volatile=True)\n",
    "    r = Variable(torch.LongTensor([r_num]), volatile=True)\n",
    "\n",
    "    res = model(c, r)[0].data.cpu().numpy()[0]\n",
    "    return [(context, response), res]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poupulate the val_cache with the use of the predict_val function and the POTENTIAL_RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Load your own data here\n",
    "INPUT_CONTEXT = \"What is the command to remove a file\"\n",
    "POTENTIAL_RESPONSES = [\"cp\", \"rm\", \"mkdir\", \"top\"]\n",
    "\n",
    "\n",
    "for response in POTENTIAL_RESPONSES:\n",
    "    print(\"\") #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exaime the val_cache dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However, in the last step we have cheated. We have manually added the candidates to be evaluated, but this is not going to be possible in a real world scenario. For that, we came up with the idea of using <b>Solr</b>. Solr gives you the opportunity (among many others that we don't need here) of indexing the whole dataset and performing similarity queries in it.\n",
    "\n",
    "## For motivated studnets\n",
    "\n",
    "The best way to perform the indexing is by creating an appropiate estructure of the data. We are going to need to query the user input (the question) against the database, select a group of the most similar existing questions and get the answer of the other user in the Ubuntu forum to be evaluated. Each sentence in the dataset can be stored with the following information:\n",
    "\n",
    "- **author**: name of the user that wrote the sentence\n",
    "- **recipient**: name of the other user present in the dialog\n",
    "- **content**: the sentence (can be considered the <i>answer</i>\n",
    "- **responseTo**: the last sentence from the other user that came before this one (can be considered the <i>question</i>)\n",
    "\n",
    "With this estructure in Solr we can query by the user question to the chatbot against the <i>responseTo</i> field of all the stored sentences. The ones with biggest Solr similarity score are the sentences that have the best probability to be asking the same questions as the user, so we can take their <i>content</i> field and add them to the set of possible answers to return to the user.\n",
    "\n",
    "Get strated with Solr here http://lucene.apache.org/solr/guide/7_5/solr-tutorial.html#exercise-1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
