{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Binary classification based on MNIST data - classifying whether a given image contains the digit '5' or not.\n",
    "A smaller ('lightweight') version of MNIST is used - containing 8x8 images that are included in the scikit-learn library.\n",
    "\n",
    "In the following, we use the following notation (see also the notations sheet):\n",
    "\n",
    "m: Number of samples <br>\n",
    "n: Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n",
      "[[ 0.  0.  1.  8. 15. 10.  0.  0.]\n",
      " [ 0.  3. 13. 15. 14. 14.  0.  0.]\n",
      " [ 0.  5. 10.  0. 10. 12.  0.  0.]\n",
      " [ 0.  0.  3.  5. 15. 10.  2.  0.]\n",
      " [ 0.  0. 16. 16. 16. 16. 12.  0.]\n",
      " [ 0.  1.  8. 12. 14.  8.  3.  0.]\n",
      " [ 0.  0.  0. 10. 13.  0.  0.  0.]\n",
      " [ 0.  0.  0. 11.  9.  0.  0.  0.]]\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "print(\"Image Data Shape\" , x.shape)\n",
    "print(\"Label Data Shape\", y.shape)\n",
    "\n",
    "image = x[17,:]\n",
    "plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "print(np.reshape(image, (8,8)))\n",
    "print(y[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use a (80%/20%) splitting.\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training set:  (64, 1437) (1, 1437)\n",
      "Shape test set:      (64, 360) (1, 360)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# reshape: \n",
    "# for x a simple transpose is sufficient \n",
    "# (m,n) -> (n,m) where m is the number of samples and n the number of input features (pixels)\n",
    "# for y reshape the simple array to become a (1,m) array\n",
    "x_train1 = x_train0.T\n",
    "x_test1 = x_test0.T\n",
    "m_train = x_train0.shape[0]\n",
    "m_test = x_test0.shape[0]\n",
    "y_train=y_train.reshape(1,m_train)\n",
    "y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "print(\"Shape training set: \", x_train1.shape, y_train.shape)\n",
    "print(\"Shape test set:     \", x_test1.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centering later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 16.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xmax = np.max(x_train1)\n",
    "xmin = np.min(x_train1)\n",
    "print(xmin, xmax)\n",
    "x_train = x_train1 / xmax\n",
    "x_test = x_test1 / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X, round=False):\n",
    "    '''\n",
    "    Compute the prediction for each of the m samples by using the parameters (w, b).\n",
    "    Return the numeric value if `round=False` - return a rounded value (0 or 1) if `round=True`. \n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array with shape (1, n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n,m)\n",
    "    round -- flag to indicate whether to round or not.\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a numpy array (vector) containing all predictions\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    print('shape w ', w.shape)\n",
    "    print('shape X ', X.shape)\n",
    "    \n",
    "    y = np.dot(w, np.transpose(X)) + b\n",
    "    #y = np.matmul(w,np.transpose(X))+b\n",
    "    if round:\n",
    "        y[y>=0] = 1.\n",
    "        y[y<0] = 0.\n",
    "        return y\n",
    "    return y\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* Cross-Entropy Cost Function\n",
    "* Mean Square Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapey(yhat,y):\n",
    "    \"\"\"\n",
    "    Checks whether the inputs come as a list in which case it reshapes it to (1,m).\n",
    "    Implementation is sloppy...\n",
    "    \"\"\"\n",
    "    if type(yhat).__module__ == np.__name__:\n",
    "        m = yhat.size\n",
    "        yhat = yhat.reshape(1,m)\n",
    "        y = y.reshape(1,m)\n",
    "    else:\n",
    "        m = 1\n",
    "    return yhat, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_CE(yhat, y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    yhat -- A scalar or numpy array with shape (1,m).\n",
    "    y    -- A scalar or numpy array with shape (1,m).\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"    \n",
    "    yhat, y, m = reshapey(yhat, y)\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    #from https://gist.github.com/Atlas7/22372a4f6b0846cfc3797766d7b529e8\n",
    "    cost = -(1.0/m) * np.sum(y*np.log(yhat) + (1-y)*np.log(1-yhat))\n",
    "    return cost\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_MSE(yhat, y):\n",
    "    \"\"\"\n",
    "    Computes the mean square error cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    yhat -- A scalar or numpy array with shape (1,m).\n",
    "    y    -- A scalar or numpy array with shape (1,m).\n",
    "    \n",
    "    Returns:\n",
    "    MSE Cost\n",
    "    \"\"\"    \n",
    "    yhat, y, m = reshapey(yhat, y)\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    cost = 1 / (2*m) * np.sum((yhat - y)**2)\n",
    "    cost2 = np.square(np.subtract(A, B)).mean()\n",
    "    if cost != cost2:\n",
    "        raise Exception('not the same costs', cost, ' ', cost2)\n",
    "    return cost\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions.\n",
    "\n",
    "![title](img/ce_update_rule.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_CE(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. w (key \"dw\") and w.r.t. b (key \"db\")\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    # see slide 56\n",
    "    n, m = X.shape\n",
    "    yhat = predict(w, b, X)\n",
    "    print('step_CE w shape ', w.shape)\n",
    "    w = (1/m) * np.dot(np.sum(yhat - Y), X) \n",
    "    \n",
    "    print('u step_CE w shape ', w.shape)\n",
    "    b = (1/m) * np.sum(yhat - Y)\n",
    "    return w, b\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see page 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_MSE(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the mean square error cost. \n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. w (key \"dw\") and w.r.t. b (key \"db\")\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    n, m = X.shape\n",
    "    yhat = predict(w, b, X)\n",
    "    w = w - b * np.dot((yhat - Y), X) \n",
    "    b = b - b * np.sum(yhat - Y)\n",
    "    return w, b\n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Output Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights of shape (1,n)\n",
    "    b -- bias (scalar)\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    return np.sum(Y !=predict(w, b, X, round=True))/Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PIXELS = (8,8)\n",
    "COLS = 5\n",
    "def plot_digits(X,Y,indices):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with up to 8 columns\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (1, 64)\n",
    "    Y -- label (a scalar)\n",
    "    indices -- list of indices    \n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        print(\"No images to show!\")\n",
    "        return\n",
    "    cols = min(COLS, len(indices))\n",
    "    rows = len(indices)/COLS+1\n",
    "    plt.figure(figsize=(20,4*rows))\n",
    "    for index, (image, label) in enumerate(zip(X.T[indices,:], Y.T[indices,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, PIXELS), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (indices[index],label), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(n, random=False):\n",
    "    \"\"\"\n",
    "    This function creates initial values:\n",
    "    * for w a vector of zeros of shape (1,n) [random=False] or a vector of normally distributed random values [random=True] \n",
    "    * for b set to 0.\n",
    "    \n",
    "    Argument:\n",
    "    n -- size of the w vector we want (number of features)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,n)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        w = np.random.randn(*(1,n))\n",
    "    else:\n",
    "        w = np.zeros((1,n))\n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, x_train, y_train, x_test, y_test, nepochs, alpha, cost_type=\"CE\"):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    Returns the trained parameters values as dictionary (keys \"w\" and \"b\") and various quantities \n",
    "    collected during learning also as dictionary: cost on training and test set (\"cost_train\", \"cost_test\"), \n",
    "    error rate on training and test set (\"error_train\", \"error_test\"), learning speed as length of dw \n",
    "    multiplied by alpha with key \"step_w\" and absolute value of db with key \"step_b\".\n",
    "    The output is provided in form of dictionaries (basically, to avoid handling too many variable names in \n",
    "    functional calls).\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    x -- data of shape (n,m)\n",
    "    y -- true \"label\" vector (containing 0 or 1), of shape (1, m)\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    cost_type -- cost function to use for the opimisation (CE: cross entropy, MSE: mean square error)\n",
    "    debug -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\"     \n",
    "    # The following lists are used for tracking the learning progress so that learning curves can be plotted.\n",
    "    # Append an according value in each epoch\n",
    "    epochs = []  # fill here the epoch id (the iteration index when looping over nepochs)\n",
    "    train_costs = [] # fill here the cost on the training set  \n",
    "    test_costs = [] # fill here the cost on the test set \n",
    "    train_errors = [] # fill here the error rate on the training set\n",
    "    test_errors = [] # fill here the error rate on the test set \n",
    "    stepsize_w = [] # fill here the lenght of the gradient of the weights vector multiplied with alpha (for the training set)\n",
    "    stepsize_b = [] # fill here the absolute value of derivative wr.t. the bias multiplied with alpha (for the training set)\n",
    "        \n",
    "    if cost_type==\"CE\":\n",
    "        step = step_CE #step_CE(w, b, X, Y) return w, b\n",
    "        cost = cost_CE #cost_CE(yhat, y) return cost\n",
    "    elif cost_type==\"MSE\":\n",
    "        step = step_MSE\n",
    "        cost = cost_MSE\n",
    "    else:\n",
    "        print(\"Cost type %s not supported.\"%cost_type)\n",
    "        return\n",
    "        \n",
    "    ### START YOUR CODE ###\n",
    "    for e in range(nepochs):\n",
    "        #epochs.append(e)\n",
    "        t_w, t_b = step(w, b, x, y)\n",
    "        w = w - alpha * t_w\n",
    "        b = b - alpha * t_b\n",
    "        epochs.append(e)\n",
    "        stepsize_w.append(w)\n",
    "        stepsize_b.append(b)\n",
    "        train_costs.append(cost(predict(w, b, x_train), y_train)) #predict(w, b, X, round=False): \n",
    "        test_costs.append(cost(predict(w, b, x_test), y_test))\n",
    "        train_errors.append(0)\n",
    "        test_errors.append(0)\n",
    "        #print(cost(y, y))\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}    \n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "        \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape w  (1, 64)\n",
      "shape X  (1797, 64)\n",
      "step_CE w shape  (1, 64)\n",
      "u step_CE w shape  (1797, 64)\n",
      "shape w  (1797, 64)\n",
      "shape X  (64, 1437)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1797,64) and (1437,64) not aligned: 64 (dim 1) != 1437 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8de32804abaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_curves\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-e43b24fe4d2c>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, x_train, y_train, x_test, y_test, nepochs, alpha, cost_type)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mstepsize_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstepsize_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#predict(w, b, X, round=False):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtest_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtrain_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-10b3080587cd>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(w, b, X, round)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape X '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m#y = np.matmul(w,np.transpose(X))+b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1797,64) and (1437,64) not aligned: 64 (dim 1) != 1437 (dim 0)"
     ]
    }
   ],
   "source": [
    "# target digit\n",
    "target = 5 \n",
    "learning_rate = 0.5\n",
    "nepochs = 20\n",
    "X_train = x_train\n",
    "Y_train = y_train==target\n",
    "X_test = x_test\n",
    "Y_test = y_test==target\n",
    "w,b = initialize_params(8*8)\n",
    "params, learning_curves = \\\n",
    "    optimize(w, b, X_train, Y_train, X_test, Y_test, nepochs=nepochs, \\\n",
    "                                 alpha = learning_rate, cost_type=\"CE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"cost_train\"], label=\"train\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"cost_test\"], label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.002,0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"error_train\"], label=\"train\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"error_test\"], label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.001,0.1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_w\"], label=\"w\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_b\"], label=\"b\")\n",
    "plt.ylabel('Step Size')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.00001,0.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_pred = predict(params['w'], params['b'], X_test, round=True)\n",
    "indices = np.where(Y_test!=Y_pred)[1]\n",
    "print(len(indices))\n",
    "print(indices)\n",
    "print(Y_test[:,indices])\n",
    "print(Y_pred[:,indices])\n",
    "print(predict(params['w'], params['b'], X_test[:,indices]))\n",
    "\n",
    "plot_digits(x_test, y_test, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as 8x8 Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(params['w'], (8,8)), cmap=plt.cm.gray)\n",
    "np.set_printoptions(precision=3, formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "print(np.reshape(params['w'], (8,8)))\n",
    "print(params['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for all the Digits\n",
    "\n",
    "Run batch gradient descent independently for all the digits (0-9).<br>\n",
    "Plot the error rates (train and test) for the trained models against the digit in a single plot. <br>\n",
    "Which digit can be predicted very well - for which is the prediction rather bad? <br>\n",
    "Plot the digits and interpret what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
