{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Binary classification based on MNIST data - classifying whether a given image contains the digit '5' or not.\n",
    "A smaller ('lightweight') version of MNIST is used - containing 8x8 images that are included in the scikit-learn library.\n",
    "\n",
    "In the following, we use the following notation (see also the notations sheet):\n",
    "\n",
    "m: Number of samples <br>\n",
    "n: Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "print(\"Image Data Shape\" , x.shape)\n",
    "print(\"Label Data Shape\", y.shape)\n",
    "\n",
    "image = x[17,:]\n",
    "plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "print(np.reshape(image, (8,8)))\n",
    "print(y[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use a (80%/20%) splitting.\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# reshape: \n",
    "# for x a simple transpose is sufficient \n",
    "# (m,n) -> (n,m) where m is the number of samples and n the number of input features (pixels)\n",
    "# for y reshape the simple array to become a (1,m) array\n",
    "x_train1 = x_train0.T\n",
    "x_test1 = x_test0.T\n",
    "m_train = x_train0.shape[0]\n",
    "m_test = x_test0.shape[0]\n",
    "y_train=y_train.reshape(1,m_train)\n",
    "y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "print(\"Shape training set: \", x_train1.shape, y_train.shape)\n",
    "print(\"Shape test set:     \", x_test1.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centering later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xmax = np.max(x_train1)\n",
    "xmin = np.min(x_train1)\n",
    "print(xmin, xmax)\n",
    "x_train = x_train1 / xmax\n",
    "x_test = x_test1 / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X, round=False):\n",
    "    '''\n",
    "    Compute the prediction for each of the m samples by using the parameters (w, b).\n",
    "    Return the numeric value if `round=False` - return a rounded value (0 or 1) if `round=True`. \n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array with shape (1, n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n,m)\n",
    "    round -- flag to indicate whether to round or not.\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a numpy array (vector) containing all predictions\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* Cross-Entropy Cost Function\n",
    "* Mean Square Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapey(yhat,y):\n",
    "    \"\"\"\n",
    "    Checks whether the inputs come as a list in which case it reshapes it to (1,m).\n",
    "    Implementation is sloppy...\n",
    "    \"\"\"\n",
    "    if type(yhat).__module__ == np.__name__:\n",
    "        m = yhat.size\n",
    "        yhat = yhat.reshape(1,m)\n",
    "        y = y.reshape(1,m)\n",
    "    else:\n",
    "        m = 1\n",
    "    return yhat, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_CE(yhat, y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    yhat -- A scalar or numpy array with shape (1,m).\n",
    "    y    -- A scalar or numpy array with shape (1,m).\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"    \n",
    "    yhat, y, m = reshapey(yhat, y)\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_MSE(yhat, y):\n",
    "    \"\"\"\n",
    "    Computes the mean square error cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    yhat -- A scalar or numpy array with shape (1,m).\n",
    "    y    -- A scalar or numpy array with shape (1,m).\n",
    "    \n",
    "    Returns:\n",
    "    MSE Cost\n",
    "    \"\"\"    \n",
    "    yhat, y, m = reshapey(yhat, y)\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_CE(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. w (key \"dw\") and w.r.t. b (key \"db\")\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_MSE(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the mean square error cost. \n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. w (key \"dw\") and w.r.t. b (key \"db\")\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Output Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights of shape (1,n)\n",
    "    b -- bias (scalar)\n",
    "    X -- data of size (n, m)\n",
    "    Y -- label vector (1, m)\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    return np.sum(Y !=predict(w, b, X, round=True))/Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PIXELS = (8,8)\n",
    "COLS = 5\n",
    "def plot_digits(X,Y,indices):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with up to 8 columns\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (1, 64)\n",
    "    Y -- label (a scalar)\n",
    "    indices -- list of indices    \n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        print(\"No images to show!\")\n",
    "        return\n",
    "    cols = min(COLS, len(indices))\n",
    "    rows = len(indices)/COLS+1\n",
    "    plt.figure(figsize=(20,4*rows))\n",
    "    for index, (image, label) in enumerate(zip(X.T[indices,:], Y.T[indices,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, PIXELS), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (indices[index],label), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(n, random=False):\n",
    "    \"\"\"\n",
    "    This function creates initial values:\n",
    "    * for w a vector of zeros of shape (1,n) [random=False] or a vector of normally distributed random values [random=True] \n",
    "    * for b set to 0.\n",
    "    \n",
    "    Argument:\n",
    "    n -- size of the w vector we want (number of features)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,n)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        w = np.random.randn(*(1,n))\n",
    "    else:\n",
    "        w = np.zeros((1,n))\n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, x_train, y_train, x_test, y_test, nepochs, alpha, cost_type=\"CE\"):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    Returns the trained parameters values as dictionary (keys \"w\" and \"b\") and various quantities \n",
    "    collected during learning also as dictionary: cost on training and test set (\"cost_train\", \"cost_test\"), \n",
    "    error rate on training and test set (\"error_train\", \"error_test\"), learning speed as length of dw \n",
    "    multiplied by alpha with key \"step_w\" and absolute value of db with key \"step_b\".\n",
    "    The output is provided in form of dictionaries (basically, to avoid handling too many variable names in \n",
    "    functional calls).\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    x -- data of shape (n,m)\n",
    "    y -- true \"label\" vector (containing 0 or 1), of shape (1, m)\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    cost_type -- cost function to use for the opimisation (CE: cross entropy, MSE: mean square error)\n",
    "    debug -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    learning_curves -- dictionary with various measures computed during the training useful for plotting \n",
    "    different learning curves.    \n",
    "    \"\"\"     \n",
    "    # The following lists are used for tracking the learning progress so that learning curves can be plotted.\n",
    "    # Append an according value in each epoch\n",
    "    epochs = []  # fill here the epoch id (the iteration index when looping over nepochs)\n",
    "    train_costs = [] # fill here the cost on the training set  \n",
    "    test_costs = [] # fill here the cost on the test set \n",
    "    train_errors = [] # fill here the error rate on the training set\n",
    "    test_errors = [] # fill here the error rate on the test set \n",
    "    stepsize_w = [] # fill here the lenght of the gradient of the weights vector multiplied with alpha (for the training set)\n",
    "    stepsize_b = [] # fill here the absolute value of derivative wr.t. the bias multiplied with alpha (for the training set)\n",
    "        \n",
    "    if cost_type==\"CE\":\n",
    "        step = step_CE\n",
    "        cost = cost_CE\n",
    "    elif cost_type==\"MSE\":\n",
    "        step = step_MSE\n",
    "        cost = cost_MSE\n",
    "    else:\n",
    "        print(\"Cost type %s not supported.\"%cost_type)\n",
    "        return\n",
    "        \n",
    "    ### START YOUR CODE ###\n",
    "        \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}    \n",
    "    learning_curves = {}\n",
    "    learning_curves[\"epochs\"] = epochs\n",
    "    learning_curves[\"step_w\"] = stepsize_w\n",
    "    learning_curves[\"step_b\"] = stepsize_b\n",
    "    learning_curves[\"cost_train\"] = train_costs\n",
    "    learning_curves[\"cost_test\"] = test_costs\n",
    "    learning_curves[\"error_train\"] = train_errors\n",
    "    learning_curves[\"error_test\"] = test_errors\n",
    "        \n",
    "    print(\"Training error / cost : %6.4f / %6.4f\"%(train_errors[-1], train_costs[-1]))\n",
    "    print(\"Test error / cost : %6.4f / %6.4f\"%(test_errors[-1], test_costs[-1]))\n",
    "\n",
    "    return params, learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target digit\n",
    "target = 5 \n",
    "learning_rate = 0.5\n",
    "nepochs = 2000\n",
    "X_train = x_train\n",
    "Y_train = y_train==target\n",
    "X_test = x_test\n",
    "Y_test = y_test==target\n",
    "w,b = initialize_params(8*8)\n",
    "params, learning_curves = \\\n",
    "    optimize(w, b, X_train, Y_train, X_test, Y_test, nepochs=nepochs, \\\n",
    "                                 alpha = learning_rate, cost_type=\"CE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"cost_train\"], label=\"train\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"cost_test\"], label=\"test\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.002,0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"error_train\"], label=\"train\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"error_test\"], label=\"test\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.001,0.1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_w\"], label=\"w\")\n",
    "plt.semilogy(learning_curves[\"epochs\"], learning_curves[\"step_b\"], label=\"b\")\n",
    "plt.ylabel('Step Size')\n",
    "plt.xlabel('Epochs')\n",
    "xmax = learning_curves[\"epochs\"][-1]\n",
    "plt.axis([0,xmax,0.00001,0.2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_pred = predict(params['w'], params['b'], X_test, round=True)\n",
    "indices = np.where(Y_test!=Y_pred)[1]\n",
    "print(len(indices))\n",
    "print(indices)\n",
    "print(Y_test[:,indices])\n",
    "print(Y_pred[:,indices])\n",
    "print(predict(params['w'], params['b'], X_test[:,indices]))\n",
    "\n",
    "plot_digits(x_test, y_test, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as 8x8 Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(params['w'], (8,8)), cmap=plt.cm.gray)\n",
    "np.set_printoptions(precision=3, formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "print(np.reshape(params['w'], (8,8)))\n",
    "print(params['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for all the Digits\n",
    "\n",
    "Run batch gradient descent independently for all the digits (0-9).<br>\n",
    "Plot the error rates (train and test) for the trained models against the digit in a single plot. <br>\n",
    "Which digit can be predicted very well - for which is the prediction rather bad? <br>\n",
    "Plot the digits and interpret what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
